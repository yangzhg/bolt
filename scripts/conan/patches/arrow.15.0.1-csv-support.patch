diff --git a/recipes/arrow/all/conandata.yml b/recipes/arrow/all/conandata.yml
index ed8cb79fd..a25132990 100644
--- a/recipes/arrow/all/conandata.yml
+++ b/recipes/arrow/all/conandata.yml
@@ -20,6 +20,9 @@ sources:
   "16.1.0":
     url: "https://www.apache.org/dyn/closer.lua/arrow/arrow-16.1.0/apache-arrow-16.1.0.tar.gz?action=download"
     sha256: "c9e60c7e87e59383d21b20dc874b17153729ee153264af6d21654b7dff2c60d7"
+  "15.0.1-oss":
+    url: "https://www.apache.org/dyn/closer.lua/arrow/arrow-15.0.1/apache-arrow-15.0.1.tar.gz?action=download"
+    sha256: "55db63ed9fd6917b7abfe5d4186c9f532cbe48aa53f4040d57e7c29ad70bcefa"
   "15.0.0":
     url: "https://www.apache.org/dyn/closer.lua/arrow/arrow-15.0.0/apache-arrow-15.0.0.tar.gz?action=download"
     sha256: "01dd3f70e85d9b5b933ec92c0db8a4ef504a5105f78d2d8622e84279fb45c25d"
@@ -58,6 +61,36 @@ patches:
     - patch_file: "patches/16.0.0-0001-fix-cmake.patch"
       patch_description: "use cci package"
       patch_type: "conan"
+  "15.0.1-oss":
+    - patch_file: "patches/15.0.0-0001-fix-cmake.patch"
+      patch_description: "do not use system openssl"
+      patch_type: "conan"
+    - patch_file: "patches/15.0.0-0001-include-assert.patch"
+      patch_description: "include assert header file to solve compile failure"
+    - patch_file: "patches/15.0.2-0001-csv-convertercc.patch"
+      patch_description: "CSV reader converter.cc patch"
+      patch_type: "conan"
+    - patch_file: "patches/15.0.2-0001-csv-converterh.patch"
+      patch_description: "CSV reader converter.h patch"
+      patch_type: "conan"
+    - patch_file: "patches/15.0.2-0001-csv-optionsh.patch"
+      patch_description: "CSV reader options.h patch"
+      patch_type: "conan"
+    - patch_file: "patches/15.0.2-0001-csv-parsercc.patch"
+      patch_description: "CSV reader parser.cc patch"
+      patch_type: "conan"
+    - patch_file: "patches/15.0.2-0001-csv-parserh.patch"
+      patch_description: "CSV reader parser.h patch"
+      patch_type: "conan"
+    - patch_file: "patches/15.0.0-0002-dataset_scan_option.patch"
+      patch_description: "csv options change"
+      patch_type: "conan"
+    - patch_file: "patches/15.0.0-0003-dataset_jni_wrapper.patch"
+      patch_description: "dataset jni_wrapper change"
+      patch_type: "conan"
+    - patch_file: "patches/15.0.0-0004-dataset_cmakelists.patch"
+      patch_description: "dataset CMakeLists.txt change"
+      patch_type: "conan"
   "15.0.0":
     - patch_file: "patches/11.0.0-0001-fix-cmake.patch"
       patch_description: "use cci package"
diff --git a/recipes/arrow/all/conanfile.py b/recipes/arrow/all/conanfile.py
index 24a7af89b..27655cdf4 100644
--- a/recipes/arrow/all/conanfile.py
+++ b/recipes/arrow/all/conanfile.py
@@ -1,15 +1,16 @@
-import os
-
 from conan import ConanFile
-from conan.errors import ConanInvalidConfiguration, ConanException
+from conan.tools import files, scm
+from conan.errors import ConanInvalidConfiguration
 from conan.tools.build import check_min_cppstd, cross_building
 from conan.tools.cmake import CMake, CMakeDeps, CMakeToolchain, cmake_layout
 from conan.tools.files import apply_conandata_patches, copy, export_conandata_patches, get, rmdir
 from conan.tools.microsoft import is_msvc, is_msvc_static_runtime
 from conan.tools.scm import Version
 
-required_conan_version = ">=2.1.0"
+import os
+import glob
 
+required_conan_version = ">=1.53.0"
 
 class ArrowConan(ConanFile):
     name = "arrow"
@@ -66,53 +67,57 @@ class ArrowConan(ConanFile):
         "with_snappy": [True, False],
         "with_zlib": [True, False],
         "with_zstd": [True, False],
+        "with_test": [True, False],
+        "with_pyarrow": [True, False]
     }
     default_options = {
         "shared": False,
         "fPIC": True,
         "gandiva": False,
-        "parquet": True,
+        "parquet": "auto",
         "skyhook": False,
         "substrait": False,
         "acero": False,
         "cli": False,
-        "compute": False,
-        "dataset_modules": False,
+        "compute": "auto",
+        "dataset_modules": "auto",
         "deprecated": True,
         "encryption": False,
-        "filesystem_layer": True,
+        "filesystem_layer": False,
         "hdfs_bridgs": False,
         "plasma": "deprecated",
         "simd_level": "default",
         "runtime_simd_level": "max",
         "with_backtrace": False,
-        "with_boost": True,
+        "with_boost": "auto",
         "with_brotli": False,
         "with_bz2": False,
         "with_csv": False,
         "with_cuda": False,
-        "with_flight_rpc": False,
+        "with_flight_rpc": "auto",
         "with_flight_sql": False,
         "with_gcs": False,
-        "with_gflags": False,
-        "with_jemalloc": False,
+        "with_gflags": "auto",
+        "with_jemalloc": "auto",
         "with_mimalloc": False,
-        "with_glog": False,
-        "with_grpc": False,
+        "with_glog": "auto",
+        "with_grpc": "auto",
         "with_json": False,
-        "with_thrift": True,
-        "with_llvm": False,
-        "with_openssl": False,
+        "with_thrift": "auto",
+        "with_llvm": "auto",
+        "with_openssl": "auto",
         "with_opentelemetry": False,
         "with_orc": False,
-        "with_protobuf": False,
-        "with_re2": False,
+        "with_protobuf": "auto",
+        "with_re2": "auto",
         "with_s3": False,
-        "with_utf8proc": False,
+        "with_utf8proc": "auto",
         "with_lz4": False,
         "with_snappy": False,
-        "with_zlib": True,
+        "with_zlib": False,
         "with_zstd": False,
+        "with_test": False,
+        "with_pyarrow": False
     }
     short_paths = True
 
@@ -120,7 +125,22 @@ class ArrowConan(ConanFile):
     def _min_cppstd(self):
         # arrow >= 10.0.0 requires C++17.
         # https://github.com/apache/arrow/pull/13991
-        return "17"
+        return "11" if Version(self.version) < "10.0.0" else "17"
+
+    @property
+    def _compilers_minimum_version(self):
+        return {
+            "11": {
+                "clang": "3.9",
+            },
+            "17": {
+                "gcc": "8",
+                "clang": "7",
+                "apple-clang": "10",
+                "Visual Studio": "15",
+                "msvc": "191",
+            },
+        }.get(self._min_cppstd, {})
 
     def export_sources(self):
         export_conandata_patches(self)
@@ -129,10 +149,35 @@ class ArrowConan(ConanFile):
     def config_options(self):
         if self.settings.os == "Windows":
             del self.options.fPIC
-        if is_msvc(self):
-            self.options.with_boost = True
-        if Version(self.version) >= "19.0.0":
-            self.options.with_mimalloc = True
+        if Version(self.version) < "2.0.0":
+            del self.options.simd_level
+            del self.options.runtime_simd_level
+        elif Version(self.version) < "6.0.0":
+            self.options.simd_level = "sse4_2"
+        if Version(self.version) < "6.0.0":
+            del self.options.with_gcs
+        if Version(self.version) < "7.0.0":
+            del self.options.skyhook
+            del self.options.with_flight_sql
+            del self.options.with_opentelemetry
+        if Version(self.version) < "8.0.0":
+            del self.options.substrait
+
+        self.options.parquet = self._parquet()
+        self.options.compute = self._compute()
+        self.options.dataset_modules = self._dataset_modules()
+        self.options.with_boost = self._with_boost()
+        self.options.with_flight_rpc = self._with_flight_rpc()
+        self.options.with_gflags = self._with_gflags()
+        self.options.with_glog = self._with_glog()
+        self.options.with_grpc = self._with_grpc()
+        self.options.with_jemalloc = self._with_jemalloc()
+        self.options.with_thrift = self._with_thrift()
+        self.options.with_llvm = self._with_llvm()
+        self.options.with_openssl = self._with_openssl()
+        self.options.with_protobuf = self._with_protobuf()
+        self.options.with_re2 = self._with_re2()
+        self.options.with_utf8proc = self._with_utf8proc()
 
     def configure(self):
         if self.options.shared:
@@ -141,15 +186,126 @@ class ArrowConan(ConanFile):
     def layout(self):
         cmake_layout(self, src_folder="src")
 
+    def _compute(self):
+        if self.options.compute == "auto":
+            return bool(self._parquet() or self._dataset_modules()) or bool(self.options.get_safe("substrait", False))
+        else:
+            return bool(self.options.compute)
+
+    def _parquet(self):
+        if self.options.parquet == "auto":
+            return bool(self.options.get_safe("substrait", False))
+        else:
+            return bool(self.options.parquet)
+
+    def _dataset_modules(self):
+        if self.options.dataset_modules == "auto":
+            return bool(self.options.get_safe("substrait", False))
+        else:
+            return bool(self.options.dataset_modules)
+
+    def _with_jemalloc(self):
+        if self.options.with_jemalloc == "auto":
+            return bool("BSD" in str(self.settings.os))
+        else:
+            return bool(self.options.with_jemalloc)
+
+    def _with_re2(self):
+        if self.options.with_re2 == "auto":
+            if self.options.gandiva or self.options.parquet:
+                return True
+            if Version(self) >= "7.0.0" and (self._compute() or self._dataset_modules()):
+                return True
+            return False
+        else:
+            return bool(self.options.with_re2)
+
+    def _with_protobuf(self):
+        if self.options.with_protobuf == "auto":
+            return bool(self.options.gandiva or self._with_flight_rpc() or self.options.with_orc or self.options.get_safe("substrait", False))
+        else:
+            return bool(self.options.with_protobuf)
+
+    def _with_flight_rpc(self):
+        if self.options.with_flight_rpc == "auto":
+            return bool(self.options.get_safe("with_flight_sql", False))
+        else:
+            return bool(self.options.with_flight_rpc)
+
+    def _with_gflags(self):
+        if self.options.with_gflags == "auto":
+            return bool(self._with_glog() or self._with_grpc())
+        else:
+            return bool(self.options.with_gflags)
+
+    def _with_glog(self):
+        if self.options.with_glog == "auto":
+            return False
+        else:
+            return bool(self.options.with_glog)
+
+    def _with_grpc(self):
+        if self.options.with_grpc == "auto":
+            return self._with_flight_rpc()
+        else:
+            return bool(self.options.with_grpc)
+
+    def _with_boost(self):
+        if self.options.with_boost == "auto":
+            if self.options.gandiva:
+                return True
+            version = Version(self.version)
+            if version.major == "1":
+                if self._parquet() and self.settings.compiler == "gcc" and self.settings.compiler.version < Version("4.9"):
+                    return True
+            elif version.major >= "2":
+                if is_msvc(self):
+                    return True
+            return False
+        else:
+            return bool(self.options.with_boost)
+
+    def _with_thrift(self):
+        if self.options.with_thrift == "auto":
+            return bool(self._parquet())
+        else:
+            return bool(self.options.with_thrift)
+
+    def _with_utf8proc(self):
+        if self.options.with_utf8proc == "auto":
+            return bool(self._compute() or self.options.gandiva)
+        else:
+            return bool(self.options.with_utf8proc)
+
+    def _with_llvm(self):
+        if self.options.with_llvm == "auto":
+            return bool(self.options.gandiva)
+        else:
+            return bool(self.options.with_llvm)
+
+    def _with_openssl(self):
+        if self.options.with_openssl == "auto":
+            return bool(self.options.encryption or self._with_flight_rpc() or self.options.with_s3)
+        else:
+            return bool(self.options.with_openssl)
+
     def _requires_rapidjson(self):
-        return (self.options.with_json or self.options.encryption or
-                (Version(self.version) >= "21.0.0" and self.options.parquet))
+        if self.options.with_json:
+            return True
+        if Version(self.version) >= "7.0.0" and self.options.encryption:
+            return True
+        if self._with_test():
+            return True
+        return False
+
+    def _with_test(self):
+        return self.options.get_safe("with_test")
 
     def requirements(self):
         if self.options.with_thrift:
-            self.requires("thrift/0.20.0")
+            self.requires("thrift/0.17.0", transitive_headers=True, transitive_libs=True)
         if self.options.with_protobuf:
-            self.requires("protobuf/3.21.12")
+            self.requires("protobuf/3.21.4", transitive_headers=True, transitive_libs=True)
         if self.options.with_jemalloc:
             self.requires("jemalloc/5.3.0")
         if self.options.with_mimalloc:
@@ -159,13 +315,13 @@ class ArrowConan(ConanFile):
         if self.options.with_gflags:
             self.requires("gflags/2.2.2")
         if self.options.with_glog:
-            self.requires("glog/0.6.0")
+            self.requires("glog/0.7.1")
         if self.options.get_safe("with_gcs"):
             self.requires("google-cloud-cpp/1.40.1")
         if self.options.with_grpc:
             self.requires("grpc/1.50.0")
         if self._requires_rapidjson():
-            self.requires("rapidjson/1.1.0")
+            self.requires("rapidjson/[>=cci.20230929]")
         if self.options.with_llvm:
             self.requires("llvm-core/13.0.0")
         if self.options.with_openssl:
@@ -178,6 +334,8 @@ class ArrowConan(ConanFile):
             self.requires("opentelemetry-cpp/1.7.0")
         if self.options.with_s3:
             self.requires("aws-sdk-cpp/1.9.234")
+        if self.options.with_orc:
+            self.requires("orc/2.0.0")
         if self.options.with_brotli:
             self.requires("brotli/1.1.0")
         if self.options.with_bz2:
@@ -185,57 +343,39 @@ class ArrowConan(ConanFile):
         if self.options.with_lz4:
             self.requires("lz4/1.9.4")
         if self.options.with_snappy:
-            self.requires("snappy/1.1.9")
-        if self.options.get_safe("simd_level") != None or \
-                self.options.get_safe("runtime_simd_level") != None:
-                self.requires("xsimd/13.0.0")
+            self.requires("snappy/1.2.1")
+        if Version(self.version) >= "6.0.0" and \
+            self.options.get_safe("simd_level") != None or \
+            self.options.get_safe("runtime_simd_level") != None:
+            self.requires("xsimd/9.0.1")
         if self.options.with_zlib:
-            self.requires("zlib/[>=1.2.11 <2]")
+            self.requires("zlib/1.2.13")
         if self.options.with_zstd:
-            self.requires("zstd/[>=1.5 <1.6]")
+            self.requires("zstd/[~1.5]")
         if self.options.with_re2:
             self.requires("re2/20230301")
         if self.options.with_utf8proc:
             self.requires("utf8proc/2.8.0")
         if self.options.with_backtrace:
             self.requires("libbacktrace/cci.20210118")
-        if self.options.with_orc:
-            self.requires("orc/2.0.0")
+        if self._with_test():
+            self.requires("gtest/1.10.0", override=True)
 
     def validate(self):
-        # Do not allow options with 'auto' value
-        # TODO: Remove "auto" from the possible values for these options
-        auto_options = [option for option, value in self.options.items() if value == "auto"]
-        if auto_options:
-            raise ConanException("Options with value 'auto' are deprecated. Please set them true/false or use its default value."
-                                 f" Please change the following options: {auto_options}")
-
-        # From https://github.com/conan-io/conan-center-index/pull/23163#issuecomment-2039808851
-        if self.options.gandiva:
-            if not self.options.with_re2:
-                raise ConanException("'with_re2' option should be True when 'gandiva=True'")
-            if not self.options.with_boost:
-                raise ConanException("'with_boost' option should be True when 'gandiva=True'")
-            if not self.options.with_utf8proc:
-                raise ConanException("'with_utf8proc' option should be True when 'gandiva=True'")
-        if self.options.with_orc:
-            if not self.options.with_lz4:
-                raise ConanException("'with_lz4' option should be True when 'orc=True'")
-            if not self.options.with_snappy:
-                raise ConanException("'with_snappy' option should be True when 'orc=True'")
-            if not self.options.with_zlib:
-                raise ConanException("'with_zlib' option should be True when 'orc=True'")
-            if not self.options.with_zstd:
-                raise ConanException("'with_zstd' option should be True when 'orc=True'")
-        if self.options.with_thrift and not self.options.with_boost:
-            raise ConanException("'with_boost' option should be True when 'thrift=True'")
-        if self.options.parquet:
-            if not self.options.with_thrift:
-                raise ConanException("'with_thrift' option should be True when 'parquet=True'")
-        if self.options.with_flight_rpc and not self.options.with_protobuf:
-            raise ConanException("'with_protobuf' option should be True when 'with_flight_rpc=True'")
+        # validate options with 'auto' as default value
+        auto_options = ["parquet", "compute", "dataset_modules", "with_boost", "with_flight_rpc", "with_gflags", "with_glog",
+                        "with_grpc", "with_jemalloc", "with_thrift", "with_llvm", "with_openssl", "with_protobuf", "with_re2", "with_utf8proc"]
+        for option in auto_options:
+            assert "auto" not in str(self.options.get_safe(option)), f"Option '{option}' contains 'auto' value, wich is not allowed. Generally the final value should be True/False"
+
+        if self.settings.compiler.get_safe("cppstd"):
+            check_min_cppstd(self, self._min_cppstd)
 
-        check_min_cppstd(self, self._min_cppstd)
+        minimum_version = self._compilers_minimum_version.get(str(self.settings.compiler), False)
+        if minimum_version and Version(self.settings.compiler.version) < minimum_version:
+            raise ConanInvalidConfiguration(
+                f"{self.ref} requires C++{self._min_cppstd}, which your compiler does not support."
+            )
 
         if self.options.get_safe("skyhook", False):
             raise ConanInvalidConfiguration("CCI has no librados recipe (yet)")
@@ -248,16 +388,11 @@ class ArrowConan(ConanFile):
             if self.dependencies["jemalloc"].options.enable_cxx:
                 raise ConanInvalidConfiguration("jemmalloc.enable_cxx of a static jemalloc must be disabled")
 
-        if self.options.with_thrift and not self.options.with_zlib:
-            raise ConanInvalidConfiguration("arrow:with_thrift requires arrow:with_zlib")
-
-        if self.options.parquet and not self.options.with_thrift:
-            raise ConanInvalidConfiguration("arrow:parquet requires arrow:with_thrift")
+        if Version(self.version) < "6.0.0" and self.options.get_safe("simd_level") == "default":
+            raise ConanInvalidConfiguration(f"In {self.ref}, simd_level options is not supported `default` value.")
 
     def build_requirements(self):
-        if Version(self.version) >= "20.0.0":
-            self.tool_requires("cmake/[>=3.25 <4]")
-        else:
+        if Version(self.version) >= "13.0.0":
             self.tool_requires("cmake/[>=3.16 <4]")
 
     def source(self):
@@ -292,7 +427,7 @@ class ArrowConan(ConanFile):
         tc.variables["ARROW_NO_DEPRECATED_API"] = not bool(self.options.deprecated)
         tc.variables["ARROW_FLIGHT"] = self.options.with_flight_rpc
         tc.variables["ARROW_FLIGHT_SQL"] = bool(self.options.get_safe("with_flight_sql", False))
-        tc.variables["ARROW_COMPUTE"] = bool(self.options.compute)
+        tc.variables["ARROW_COMPUTE"] = self.options.compute
         tc.variables["ARROW_CSV"] = bool(self.options.with_csv)
         tc.variables["ARROW_CUDA"] = bool(self.options.with_cuda)
         tc.variables["ARROW_JEMALLOC"] = self.options.with_jemalloc
@@ -313,7 +448,6 @@ class ArrowConan(ConanFile):
         tc.variables["GLOG_SOURCE"] = "SYSTEM"
         tc.variables["ARROW_WITH_BACKTRACE"] = bool(self.options.with_backtrace)
         tc.variables["ARROW_WITH_BROTLI"] = bool(self.options.with_brotli)
-        tc.variables["ARROW_WITH_RE2"] = bool(self.options.with_re2)
         tc.variables["brotli_SOURCE"] = "SYSTEM"
         if self.options.with_brotli:
             tc.variables["ARROW_BROTLI_USE_SHARED"] = bool(self.dependencies["brotli"].options.shared)
@@ -338,18 +472,19 @@ class ArrowConan(ConanFile):
         tc.variables["ZLIB_SOURCE"] = "SYSTEM"
         tc.variables["xsimd_SOURCE"] = "SYSTEM"
         tc.variables["ARROW_WITH_ZSTD"] = bool(self.options.with_zstd)
-        tc.variables["zstd_SOURCE"] = "SYSTEM"
-        tc.variables["ARROW_SIMD_LEVEL"] = str(self.options.simd_level).upper()
-        tc.variables["ARROW_RUNTIME_SIMD_LEVEL"] = str(self.options.runtime_simd_level).upper()
+        if Version(self.version) >= "2.0":
+            tc.variables["zstd_SOURCE"] = "SYSTEM"
+            tc.variables["ARROW_SIMD_LEVEL"] = str(self.options.simd_level).upper()
+            tc.variables["ARROW_RUNTIME_SIMD_LEVEL"] = str(self.options.runtime_simd_level).upper()
+        else:
+            tc.variables["ZSTD_SOURCE"] = "SYSTEM"
         if self.options.with_zstd:
             tc.variables["ARROW_ZSTD_USE_SHARED"] = bool(self.dependencies["zstd"].options.shared)
         tc.variables["ORC_SOURCE"] = "SYSTEM"
         tc.variables["ARROW_ORC"] = bool(self.options.with_orc)
         tc.variables["ARROW_WITH_THRIFT"] = bool(self.options.with_thrift)
-        tc.variables["ARROW_THRIFT"] = bool(self.options.with_thrift)
         tc.variables["Thrift_SOURCE"] = "SYSTEM"
         if self.options.with_thrift:
-            tc.variables["ARROW_THRIFT"] = True
             tc.variables["THRIFT_VERSION"] = bool(self.dependencies["thrift"].ref.version) # a recent thrift does not require boost
             tc.variables["ARROW_THRIFT_USE_SHARED"] = bool(self.dependencies["thrift"].options.shared)
         tc.variables["ARROW_USE_OPENSSL"] = self.options.with_openssl
@@ -379,26 +514,47 @@ class ArrowConan(ConanFile):
             tc.variables["ARROW_USE_STATIC_CRT"] = is_msvc_static_runtime(self)
         if self.options.with_llvm:
             tc.variables["LLVM_DIR"] = self.dependencies["llvm-core"].package_folder.replace("\\", "/")
+        tc.variables["ARROW_PYTHON"] = bool(self.options.with_pyarrow)
+        if self._with_test():
+            tc.variables["ARROW_TESTING"] = "ON"
+            tc.variables["GTest_SOURCE"] = "BUNDLED"
 
         tc.cache_variables["CMAKE_PROJECT_arrow_INCLUDE"] = os.path.join(self.source_folder, "conan_cmake_project_include.cmake")
         tc.generate()
 
         deps = CMakeDeps(self)
-        deps.set_property("mimalloc", "cmake_target_name", "mimalloc::mimalloc")
         deps.generate()
 
     def _patch_sources(self):
         apply_conandata_patches(self)
+        if "7.0.0" <= Version(self.version) < "10.0.0":
+            for filename in glob.glob(os.path.join(self.source_folder, "cpp", "cmake_modules", "Find*.cmake")):
+                if os.path.basename(filename) not in [
+                    "FindArrow.cmake",
+                    "FindArrowAcero.cmake",
+                    "FindArrowCUDA.cmake",
+                    "FindArrowDataset.cmake",
+                    "FindArrowFlight.cmake",
+                    "FindArrowFlightSql.cmake",
+                    "FindArrowFlightTesting.cmake",
+                    "FindArrowPython.cmake",
+                    "FindArrowPythonFlight.cmake",
+                    "FindArrowSubstrait.cmake",
+                    "FindArrowTesting.cmake",
+                    "FindGandiva.cmake",
+                    "FindParquet.cmake",
+                ]:
+                    os.remove(filename)
 
     def build(self):
-        cmake = CMake(self)
+        cmake =CMake(self)
         cmake.configure(build_script_folder=os.path.join(self.source_folder, "cpp"))
         cmake.build()
 
     def package(self):
         copy(self, pattern="LICENSE.txt", dst=os.path.join(self.package_folder, "licenses"), src=self.source_folder)
         copy(self, pattern="NOTICE.txt", dst=os.path.join(self.package_folder, "licenses"), src=self.source_folder)
-        cmake = CMake(self)
+        cmake =CMake(self)
         cmake.install()
 
         rmdir(self, os.path.join(self.package_folder, "lib", "cmake"))
@@ -411,10 +567,8 @@ class ArrowConan(ConanFile):
         self.cpp_info.set_property("cmake_file_name", "Arrow")
 
         suffix = "_static" if is_msvc(self) and not self.options.shared else ""
-        cmake_suffix = "shared" if self.options.shared else "static"
 
         self.cpp_info.components["libarrow"].set_property("pkg_config_name", "arrow")
-        self.cpp_info.components["libarrow"].set_property("cmake_target_name", f"Arrow::arrow_{cmake_suffix}")
         self.cpp_info.components["libarrow"].libs = [f"arrow{suffix}"]
         if not self.options.shared:
             self.cpp_info.components["libarrow"].defines = ["ARROW_STATIC"]
@@ -423,7 +577,6 @@ class ArrowConan(ConanFile):
 
         if self.options.parquet:
             self.cpp_info.components["libparquet"].set_property("pkg_config_name", "parquet")
-            self.cpp_info.components["libparquet"].set_property("cmake_target_name", f"Parquet::parquet_{cmake_suffix}")
             self.cpp_info.components["libparquet"].libs = [f"parquet{suffix}"]
             self.cpp_info.components["libparquet"].requires = ["libarrow"]
             if not self.options.shared:
@@ -431,7 +584,6 @@ class ArrowConan(ConanFile):
 
         if self.options.get_safe("substrait"):
             self.cpp_info.components["libarrow_substrait"].set_property("pkg_config_name", "arrow_substrait")
-            self.cpp_info.components["libarrow_substrait"].set_property("cmake_target_name", f"Arrow::arrow_substrait_{cmake_suffix}")
             self.cpp_info.components["libarrow_substrait"].libs = [f"arrow_substrait{suffix}"]
             self.cpp_info.components["libarrow_substrait"].requires = ["libparquet", "dataset"]
 
@@ -439,26 +591,14 @@ class ArrowConan(ConanFile):
         del self.options.plasma
 
         if self.options.acero:
-            self.cpp_info.components["libacero"].set_property("pkg_config_name", "acero")
-            self.cpp_info.components["libacero"].set_property("cmake_target_name", f"Acero::arrow_acero_{cmake_suffix}")
             self.cpp_info.components["libacero"].libs = [f"arrow_acero{suffix}"]
             self.cpp_info.components["libacero"].names["cmake_find_package"] = "acero"
             self.cpp_info.components["libacero"].names["cmake_find_package_multi"] = "acero"
             self.cpp_info.components["libacero"].names["pkg_config"] = "acero"
             self.cpp_info.components["libacero"].requires = ["libarrow"]
-            if Version(self.version) >= "21.0.0" and self.options.compute:
-                # libacero depends on compute
-                self.cpp_info.components["libacero"].requires.append("libarrow_compute")
-
-        if Version(self.version) >= "21.0.0" and self.options.compute:
-            self.cpp_info.components["libarrow_compute"].set_property("pkg_config_name", "arrow_compute")
-            self.cpp_info.components["libarrow_compute"].set_property("cmake_target_name", f"ArrowCompute::arrow_compute_{cmake_suffix}")
-            self.cpp_info.components["libarrow_compute"].libs = [f"arrow_compute{suffix}"]
-            self.cpp_info.components["libarrow_compute"].requires = ["libarrow"]
 
         if self.options.gandiva:
             self.cpp_info.components["libgandiva"].set_property("pkg_config_name", "gandiva")
-            self.cpp_info.components["libgandiva"].set_property("cmake_target_name", f"Gandiva::gandiva_{cmake_suffix}")
             self.cpp_info.components["libgandiva"].libs = [f"gandiva{suffix}"]
             self.cpp_info.components["libgandiva"].requires = ["libarrow"]
             if not self.options.shared:
@@ -466,16 +606,11 @@ class ArrowConan(ConanFile):
 
         if self.options.with_flight_rpc:
             self.cpp_info.components["libarrow_flight"].set_property("pkg_config_name", "flight_rpc")
-            self.cpp_info.components["libarrow_flight"].set_property("cmake_target_name", f"ArrowFlight::arrow_flight_{cmake_suffix}")
             self.cpp_info.components["libarrow_flight"].libs = [f"arrow_flight{suffix}"]
             self.cpp_info.components["libarrow_flight"].requires = ["libarrow"]
-            # https://github.com/apache/arrow/pull/43137#pullrequestreview-2267476893
-            if Version(self.version) >= "18.0.0" and self.options.with_openssl:
-                self.cpp_info.components["libarrow_flight"].requires.append("openssl::openssl")
 
         if self.options.get_safe("with_flight_sql"):
             self.cpp_info.components["libarrow_flight_sql"].set_property("pkg_config_name", "flight_sql")
-            self.cpp_info.components["libarrow_flight_sql"].set_property("cmake_target_name", f"ArrowFlightSql::arrow_flight_sql_{cmake_suffix}")
             self.cpp_info.components["libarrow_flight_sql"].libs = [f"arrow_flight_sql{suffix}"]
             self.cpp_info.components["libarrow_flight_sql"].requires = ["libarrow", "libarrow_flight"]
 
@@ -483,8 +618,6 @@ class ArrowConan(ConanFile):
             self.cpp_info.components["dataset"].libs = ["arrow_dataset"]
             if self.options.parquet:
                 self.cpp_info.components["dataset"].requires = ["libparquet"]
-            if self.options.acero and Version(self.version) >= "19.0.0":
-                self.cpp_info.components["dataset"].requires = ["libacero"]
 
         if self.options.cli and (self.options.with_cuda or self.options.with_flight_rpc or self.options.parquet):
             binpath = os.path.join(self.package_folder, "bin")
@@ -497,8 +630,9 @@ class ArrowConan(ConanFile):
                 self.cpp_info.components["libgandiva"].requires.append("boost::boost")
             if self.options.parquet and self.settings.compiler == "gcc" and self.settings.compiler.version < Version("4.9"):
                 self.cpp_info.components["libparquet"].requires.append("boost::boost")
-            # FIXME: only headers components is used
-            self.cpp_info.components["libarrow"].requires.append("boost::boost")
+            if Version(self.version) >= "2.0":
+                # FIXME: only headers components is used
+                self.cpp_info.components["libarrow"].requires.append("boost::boost")
         if self.options.with_openssl:
             self.cpp_info.components["libarrow"].requires.append("openssl::openssl")
         if self.options.with_gflags:
@@ -530,8 +664,7 @@ class ArrowConan(ConanFile):
         if self._requires_rapidjson():
             self.cpp_info.components["libarrow"].requires.append("rapidjson::rapidjson")
         if self.options.with_s3:
-            # https://github.com/apache/arrow/blob/6b268f62a8a172249ef35f093009c740c32e1f36/cpp/src/arrow/CMakeLists.txt#L98
-            self.cpp_info.components["libarrow"].requires.extend([f"aws-sdk-cpp::{x}" for x in ["cognito-identity", "core", "identity-management", "s3", "sts"]])
+            self.cpp_info.components["libarrow"].requires.append("aws-sdk-cpp::s3")
         if self.options.get_safe("with_gcs"):
             self.cpp_info.components["libarrow"].requires.append("google-cloud-cpp::storage")
         if self.options.with_orc:
@@ -552,7 +685,37 @@ class ArrowConan(ConanFile):
             self.cpp_info.components["libarrow"].requires.append("zlib::zlib")
         if self.options.with_zstd:
             self.cpp_info.components["libarrow"].requires.append("zstd::zstd")
+        if self.options.with_boost:
+            self.cpp_info.components["libarrow"].requires.append("boost::boost")
         if self.options.with_grpc:
             self.cpp_info.components["libarrow"].requires.append("grpc::grpc")
         if self.options.with_flight_rpc:
             self.cpp_info.components["libarrow_flight"].requires.append("protobuf::protobuf")
+
+        if self._with_test():
+            self.cpp_info.components["libarrow_testing"].set_property("pkg_config_name", "arrow_testing")
+            self.cpp_info.components["libarrow_testing"].libs = [f"arrow_testing"]
+            self.cpp_info.components["libarrow_testing"].requires = ["libarrow"]
+
+        # TODO: to remove in conan v2
+        
+        
+        self.cpp_info.components["libarrow"].names["cmake_find_package"] = "arrow"
+        self.cpp_info.components["libarrow"].names["cmake_find_package_multi"] = "arrow"
+        if self.options.parquet:
+            self.cpp_info.components["libparquet"].names["cmake_find_package"] = "parquet"
+            self.cpp_info.components["libparquet"].names["cmake_find_package_multi"] = "parquet"
+        if self.options.get_safe("substrait"):
+            self.cpp_info.components["libarrow_substrait"].names["cmake_find_package"] = "arrow_substrait"
+            self.cpp_info.components["libarrow_substrait"].names["cmake_find_package_multi"] = "arrow_substrait"
+        if self.options.gandiva:
+            self.cpp_info.components["libgandiva"].names["cmake_find_package"] = "gandiva"
+            self.cpp_info.components["libgandiva"].names["cmake_find_package_multi"] = "gandiva"
+        if self.options.with_flight_rpc:
+            self.cpp_info.components["libarrow_flight"].names["cmake_find_package"] = "flight_rpc"
+            self.cpp_info.components["libarrow_flight"].names["cmake_find_package_multi"] = "flight_rpc"
+        if self.options.get_safe("with_flight_sql"):
+            self.cpp_info.components["libarrow_flight_sql"].names["cmake_find_package"] = "flight_sql"
+            self.cpp_info.components["libarrow_flight_sql"].names["cmake_find_package_multi"] = "flight_sql"
+        if self.options.cli and (self.options.with_cuda or self.options.with_flight_rpc or self.options.parquet):
+            self.env_info.PATH.append(os.path.join(self.package_folder, "bin"))
diff --git a/recipes/arrow/all/patches/15.0.0-0001-fix-cmake.patch b/recipes/arrow/all/patches/15.0.0-0001-fix-cmake.patch
new file mode 100644
index 000000000..6f24d5ed6
--- /dev/null
+++ b/recipes/arrow/all/patches/15.0.0-0001-fix-cmake.patch
@@ -0,0 +1,13 @@
+diff --git a/cpp/src/parquet/CMakeLists.txt b/cpp/src/parquet/CMakeLists.txt
+index 04028431b..fcf8f0f5f 100644
+--- a/cpp/src/parquet/CMakeLists.txt
++++ b/cpp/src/parquet/CMakeLists.txt
+@@ -228,6 +228,8 @@ if(ARROW_HAVE_RUNTIME_AVX2)
+ endif()
+ 
+ if(PARQUET_REQUIRE_ENCRYPTION)
++  list(APPEND PARQUET_SHARED_PRIVATE_LINK_LIBS ${ARROW_OPENSSL_LIBS})
++  list(APPEND PARQUET_STATIC_LINK_LIBS ${ARROW_OPENSSL_LIBS})
+   set(PARQUET_SRCS ${PARQUET_SRCS} encryption/encryption_internal.cc
+                    encryption/openssl_internal.cc)
+   # Encryption key management
diff --git a/recipes/arrow/all/patches/15.0.0-0001-include-assert.patch b/recipes/arrow/all/patches/15.0.0-0001-include-assert.patch
new file mode 100644
index 000000000..1a93f8c58
--- /dev/null
+++ b/recipes/arrow/all/patches/15.0.0-0001-include-assert.patch
@@ -0,0 +1,12 @@
+diff --git a/cpp/src/arrow/c/helpers.h b/cpp/src/arrow/c/helpers.h
+index a24f272fe..5bf6a593d 100644
+--- a/cpp/src/arrow/c/helpers.h
++++ b/cpp/src/arrow/c/helpers.h
+@@ -17,6 +17,7 @@
+ 
+ #pragma once
+ 
++#include <assert.h>
+ #include <stdio.h>
+ #include <stdlib.h>
+ #include <string.h>
diff --git a/recipes/arrow/all/patches/15.0.0-0002-dataset_scan_option.patch b/recipes/arrow/all/patches/15.0.0-0002-dataset_scan_option.patch
new file mode 100644
index 000000000..ec5a027bc
--- /dev/null
+++ b/recipes/arrow/all/patches/15.0.0-0002-dataset_scan_option.patch
@@ -0,0 +1,883 @@
+diff --git a/cpp/src/arrow/dataset/file_csv.cc b/cpp/src/arrow/dataset/file_csv.cc
+index 09ab775..f09377c 100644
+--- a/cpp/src/arrow/dataset/file_csv.cc
++++ b/cpp/src/arrow/dataset/file_csv.cc
+@@ -24,6 +24,7 @@
+ #include <unordered_set>
+ #include <utility>
+ 
++#include "arrow/c/bridge.h"
+ #include "arrow/csv/options.h"
+ #include "arrow/csv/parser.h"
+ #include "arrow/csv/reader.h"
+@@ -52,6 +53,9 @@ using internal::Executor;
+ using internal::SerialExecutor;
+ 
+ namespace dataset {
++namespace {
++inline bool parseBool(const std::string& value) { return value == "true" ? true : false; }
++}  // namespace
+ 
+ struct CsvInspectedFragment : public InspectedFragment {
+   CsvInspectedFragment(std::vector<std::string> column_names,
+@@ -503,5 +507,33 @@ Future<> CsvFileWriter::FinishInternal() {
+   return Status::OK();
+ }
+ 
++Result<std::shared_ptr<FragmentScanOptions>> CsvFragmentScanOptions::from(
++    const std::unordered_map<std::string, std::string>& configs) {
++  std::shared_ptr<CsvFragmentScanOptions> options =
++      std::make_shared<CsvFragmentScanOptions>();
++  for (auto const& it : configs) {
++    auto& key = it.first;
++    auto& value = it.second;
++    if (key == "delimiter") {
++      options->parse_options.delimiter = value.data()[0];
++    } else if (key == "quoting") {
++      options->parse_options.quoting = parseBool(value);
++    } else if (key == "column_types") {
++      int64_t schema_address = std::stol(value);
++      ArrowSchema* cSchema = reinterpret_cast<ArrowSchema*>(schema_address);
++      ARROW_ASSIGN_OR_RAISE(auto schema, arrow::ImportSchema(cSchema));
++      auto& column_types = options->convert_options.column_types;
++      for (auto field : schema->fields()) {
++        column_types[field->name()] = field->type();
++      }
++    } else if (key == "strings_can_be_null") {
++      options->convert_options.strings_can_be_null = parseBool(value);
++    } else {
++      return Status::Invalid("Config " + it.first + "is not supported.");
++    }
++  }
++  return options;
++}
++
+ }  // namespace dataset
+ }  // namespace arrow
+diff --git a/cpp/src/arrow/dataset/file_csv.h b/cpp/src/arrow/dataset/file_csv.h
+index 42e3fd7..4d28251 100644
+--- a/cpp/src/arrow/dataset/file_csv.h
++++ b/cpp/src/arrow/dataset/file_csv.h
+@@ -85,6 +85,9 @@ class ARROW_DS_EXPORT CsvFileFormat : public FileFormat {
+ struct ARROW_DS_EXPORT CsvFragmentScanOptions : public FragmentScanOptions {
+   std::string type_name() const override { return kCsvTypeName; }
+ 
++  static Result<std::shared_ptr<FragmentScanOptions>> from(
++      const std::unordered_map<std::string, std::string>& configs);
++
+   using StreamWrapFunc = std::function<Result<std::shared_ptr<io::InputStream>>(
+       std::shared_ptr<io::InputStream>)>;
+ 
+diff --git a/cpp/src/arrow/engine/substrait/expression_internal.cc b/cpp/src/arrow/engine/substrait/expression_internal.cc
+index 5d892af..0f8b044 100644
+--- a/cpp/src/arrow/engine/substrait/expression_internal.cc
++++ b/cpp/src/arrow/engine/substrait/expression_internal.cc
+@@ -1337,5 +1337,17 @@ Result<std::unique_ptr<substrait::Expression>> ToProto(
+   return std::move(out);
+ }
+ 
++Status FromProto(const substrait::Expression::Literal& literal,
++                 std::unordered_map<std::string, std::string>& out) {
++  ARROW_RETURN_IF(!literal.has_map(), Status::Invalid("Literal does not have a map."));
++  auto literalMap = literal.map();
++  auto size = literalMap.key_values_size();
++  for (auto i = 0; i < size; i++) {
++    substrait::Expression_Literal_Map_KeyValue keyValue = literalMap.key_values(i);
++    out.emplace(keyValue.key().string(), keyValue.value().string());
++  }
++  return Status::OK();
++}
++
+ }  // namespace engine
+ }  // namespace arrow
+diff --git a/cpp/src/arrow/engine/substrait/expression_internal.h b/cpp/src/arrow/engine/substrait/expression_internal.h
+index 2ce2ee7..9be81b7 100644
+--- a/cpp/src/arrow/engine/substrait/expression_internal.h
++++ b/cpp/src/arrow/engine/substrait/expression_internal.h
+@@ -61,5 +61,9 @@ ARROW_ENGINE_EXPORT
+ Result<SubstraitCall> FromProto(const substrait::AggregateFunction&, bool is_hash,
+                                 const ExtensionSet&, const ConversionOptions&);
+ 
++ARROW_ENGINE_EXPORT
++Status FromProto(const substrait::Expression::Literal& literal,
++                 std::unordered_map<std::string, std::string>& out);
++
+ }  // namespace engine
+ }  // namespace arrow
+diff --git a/cpp/src/arrow/engine/substrait/serde.cc b/cpp/src/arrow/engine/substrait/serde.cc
+index 9e670f1..02e5c71 100644
+--- a/cpp/src/arrow/engine/substrait/serde.cc
++++ b/cpp/src/arrow/engine/substrait/serde.cc
+@@ -247,6 +247,16 @@ Result<BoundExpressions> DeserializeExpressions(
+   return FromProto(extended_expression, ext_set_out, conversion_options, registry);
+ }
+ 
++Status DeserializeMap(const Buffer& buf,
++                      std::unordered_map<std::string, std::string>& out) {
++  // ARROW_ASSIGN_OR_RAISE(auto advanced_extension,
++  //                       ParseFromBuffer<substrait::extensions::AdvancedExtension>(buf));
++  // return FromProto(advanced_extension, out);
++  ARROW_ASSIGN_OR_RAISE(auto literal,
++                        ParseFromBuffer<substrait::Expression::Literal>(buf));
++  return FromProto(literal, out);
++}
++
+ namespace {
+ 
+ Result<std::shared_ptr<acero::ExecPlan>> MakeSingleDeclarationPlan(
+diff --git a/cpp/src/arrow/engine/substrait/serde.h b/cpp/src/arrow/engine/substrait/serde.h
+index ab749f4..6312ec2 100644
+--- a/cpp/src/arrow/engine/substrait/serde.h
++++ b/cpp/src/arrow/engine/substrait/serde.h
+@@ -23,6 +23,7 @@
+ #include <memory>
+ #include <string>
+ #include <string_view>
++#include <unordered_map>
+ #include <vector>
+ 
+ #include "arrow/compute/type_fwd.h"
+@@ -183,6 +184,9 @@ ARROW_ENGINE_EXPORT Result<BoundExpressions> DeserializeExpressions(
+     const ConversionOptions& conversion_options = {},
+     ExtensionSet* ext_set_out = NULLPTR);
+ 
++ARROW_ENGINE_EXPORT Status
++DeserializeMap(const Buffer& buf, std::unordered_map<std::string, std::string>& out);
++
+ /// \brief Deserializes a Substrait Type message to the corresponding Arrow type
+ ///
+ /// \param[in] buf a buffer containing the protobuf serialization of a Substrait Type
+diff --git a/java/dataset/pom.xml b/java/dataset/pom.xml
+index f5caeeb..7229863 100644
+--- a/java/dataset/pom.xml
++++ b/java/dataset/pom.xml
+@@ -25,9 +25,10 @@
+     <packaging>jar</packaging>
+     <properties>
+         <arrow.cpp.build.dir>../../../cpp/release-build/</arrow.cpp.build.dir>
+-        <protobuf.version>2.5.0</protobuf.version>
+         <parquet.version>1.11.0</parquet.version>
+         <avro.version>1.11.3</avro.version>
++        <substrait.version>0.31.0</substrait.version>
++        <protobuf.version>3.25.3</protobuf.version>
+     </properties>
+ 
+     <dependencies>
+@@ -47,6 +48,18 @@
+             <artifactId>arrow-c-data</artifactId>
+             <scope>compile</scope>
+         </dependency>
++        <dependency>
++            <groupId>io.substrait</groupId>
++            <artifactId>core</artifactId>
++            <version>${substrait.version}</version>
++            <scope>provided</scope>
++        </dependency>
++        <dependency>
++            <groupId>com.google.protobuf</groupId>
++            <artifactId>protobuf-java</artifactId>
++            <version>${protobuf.version}</version>
++            <scope>provided</scope>
++        </dependency>
+         <dependency>
+             <groupId>org.apache.arrow</groupId>
+             <artifactId>arrow-memory-netty</artifactId>
+diff --git a/java/dataset/src/main/cpp/jni_wrapper.cc b/java/dataset/src/main/cpp/jni_wrapper.cc
+index d2d9766..36cdd8d 100644
+--- a/java/dataset/src/main/cpp/jni_wrapper.cc
++++ b/java/dataset/src/main/cpp/jni_wrapper.cc
+@@ -25,6 +25,7 @@
+ #include "arrow/c/helpers.h"
+ #include "arrow/dataset/api.h"
+ #include "arrow/dataset/file_base.h"
++#include "arrow/dataset/file_csv.h"
+ #include "arrow/filesystem/localfs.h"
+ #include "arrow/filesystem/path_util.h"
+ #include "arrow/filesystem/s3fs.h"
+@@ -120,6 +121,19 @@ arrow::Result<std::shared_ptr<arrow::dataset::FileFormat>> GetFileFormat(
+   }
+ }
+ 
++arrow::Result<std::shared_ptr<arrow::dataset::FragmentScanOptions>>
++GetFragmentScanOptions(jint file_format_id,
++                       const std::unordered_map<std::string, std::string>& configs) {
++  switch (file_format_id) {
++#ifdef ARROW_CSV
++    case 3:
++      return arrow::dataset::CsvFragmentScanOptions::from(configs);
++#endif
++    default:
++      return arrow::Status::Invalid("Illegal file format id: " ,file_format_id);
++  }
++}
++
+ class ReserveFromJava : public arrow::dataset::jni::ReservationListener {
+  public:
+   ReserveFromJava(JavaVM* vm, jobject java_reservation_listener)
+@@ -464,12 +478,13 @@ JNIEXPORT void JNICALL Java_org_apache_arrow_dataset_jni_JniWrapper_closeDataset
+ /*
+  * Class:     org_apache_arrow_dataset_jni_JniWrapper
+  * Method:    createScanner
+- * Signature: (J[Ljava/lang/String;Ljava/nio/ByteBuffer;Ljava/nio/ByteBuffer;JJ)J
++ * Signature:
++ * (J[Ljava/lang/String;Ljava/nio/ByteBuffer;Ljava/nio/ByteBuffer;JJ;Ljava/nio/ByteBuffer;J)J
+  */
+ JNIEXPORT jlong JNICALL Java_org_apache_arrow_dataset_jni_JniWrapper_createScanner(
+     JNIEnv* env, jobject, jlong dataset_id, jobjectArray columns,
+-    jobject substrait_projection, jobject substrait_filter,
+-    jlong batch_size, jlong memory_pool_id) {
++    jobject substrait_projection, jobject substrait_filter, jlong batch_size,
++    jlong file_format_id, jobject options, jlong memory_pool_id) {
+   JNI_METHOD_START
+   arrow::MemoryPool* pool = reinterpret_cast<arrow::MemoryPool*>(memory_pool_id);
+   if (pool == nullptr) {
+@@ -518,6 +533,14 @@ JNIEXPORT jlong JNICALL Java_org_apache_arrow_dataset_jni_JniWrapper_createScann
+     }
+     JniAssertOkOrThrow(scanner_builder->Filter(*filter_expr));
+   }
++  if (file_format_id != -1 && options != nullptr) {
++    std::unordered_map<std::string, std::string> option_map;
++    std::shared_ptr<arrow::Buffer> buffer = LoadArrowBufferFromByteBuffer(env, options);
++    JniAssertOkOrThrow(arrow::engine::DeserializeMap(*buffer, option_map));
++    std::shared_ptr<arrow::dataset::FragmentScanOptions> scan_options =
++        JniGetOrThrow(GetFragmentScanOptions(file_format_id, option_map));
++    JniAssertOkOrThrow(scanner_builder->FragmentScanOptions(scan_options));
++  }
+   JniAssertOkOrThrow(scanner_builder->BatchSize(batch_size));
+ 
+   auto scanner = JniGetOrThrow(scanner_builder->Finish());
+@@ -629,14 +652,31 @@ JNIEXPORT void JNICALL Java_org_apache_arrow_dataset_jni_JniWrapper_ensureS3Fina
+ /*
+  * Class:     org_apache_arrow_dataset_file_JniWrapper
+  * Method:    makeFileSystemDatasetFactory
+- * Signature: (Ljava/lang/String;II)J
++ * Signature: (Ljava/lang/String;IILjava/lang/String;Ljava/nio/ByteBuffer)J
+  */
+ JNIEXPORT jlong JNICALL
+-Java_org_apache_arrow_dataset_file_JniWrapper_makeFileSystemDatasetFactory__Ljava_lang_String_2I(
+-    JNIEnv* env, jobject, jstring uri, jint file_format_id) {
++Java_org_apache_arrow_dataset_file_JniWrapper_makeFileSystemDatasetFactory(
++    JNIEnv* env, jobject, jstring uri, jint file_format_id, jobject options) {
+   JNI_METHOD_START
+   std::shared_ptr<arrow::dataset::FileFormat> file_format =
+       JniGetOrThrow(GetFileFormat(file_format_id));
++  if (options != nullptr) {
++    std::unordered_map<std::string, std::string> option_map;
++    std::shared_ptr<arrow::Buffer> buffer = LoadArrowBufferFromByteBuffer(env, options);
++    JniAssertOkOrThrow(arrow::engine::DeserializeMap(*buffer, option_map));
++    std::shared_ptr<arrow::dataset::FragmentScanOptions> scan_options =
++        JniGetOrThrow(GetFragmentScanOptions(file_format_id, option_map));
++    file_format->default_fragment_scan_options = scan_options;
++#ifdef ARROW_CSV
++    if (file_format_id == 3) {
++      std::shared_ptr<arrow::dataset::CsvFileFormat> csv_file_format =
++          std::dynamic_pointer_cast<arrow::dataset::CsvFileFormat>(file_format);
++      csv_file_format->parse_options =
++          std::dynamic_pointer_cast<arrow::dataset::CsvFragmentScanOptions>(scan_options)
++              ->parse_options;
++    }
++#endif
++  }
+   arrow::dataset::FileSystemFactoryOptions options;
+   std::shared_ptr<arrow::dataset::DatasetFactory> d =
+       JniGetOrThrow(arrow::dataset::FileSystemDatasetFactory::Make(
+@@ -647,16 +687,33 @@ Java_org_apache_arrow_dataset_file_JniWrapper_makeFileSystemDatasetFactory__Ljav
+ 
+ /*
+  * Class:     org_apache_arrow_dataset_file_JniWrapper
+- * Method:    makeFileSystemDatasetFactory
+- * Signature: ([Ljava/lang/String;II)J
++ * Method:    makeFileSystemDatasetFactoryWithFiles
++ * Signature: ([Ljava/lang/String;IIJ;Ljava/nio/ByteBuffer)J
+  */
+ JNIEXPORT jlong JNICALL
+-Java_org_apache_arrow_dataset_file_JniWrapper_makeFileSystemDatasetFactory___3Ljava_lang_String_2I(
+-    JNIEnv* env, jobject, jobjectArray uris, jint file_format_id) {
++Java_org_apache_arrow_dataset_file_JniWrapper_makeFileSystemDatasetFactoryWithFiles(
++    JNIEnv* env, jobject, jobjectArray uris, jint file_format_id, jobject options) {
+   JNI_METHOD_START
+ 
+   std::shared_ptr<arrow::dataset::FileFormat> file_format =
+       JniGetOrThrow(GetFileFormat(file_format_id));
++  if (options != nullptr) {
++    std::unordered_map<std::string, std::string> option_map;
++    std::shared_ptr<arrow::Buffer> buffer = LoadArrowBufferFromByteBuffer(env, options);
++    JniAssertOkOrThrow(arrow::engine::DeserializeMap(*buffer, option_map));
++    std::shared_ptr<arrow::dataset::FragmentScanOptions> scan_options =
++        JniGetOrThrow(GetFragmentScanOptions(file_format_id, option_map));
++    file_format->default_fragment_scan_options = scan_options;
++#ifdef ARROW_CSV
++    if (file_format_id == 3) {
++      std::shared_ptr<arrow::dataset::CsvFileFormat> csv_file_format =
++          std::dynamic_pointer_cast<arrow::dataset::CsvFileFormat>(file_format);
++      csv_file_format->parse_options =
++          std::dynamic_pointer_cast<arrow::dataset::CsvFragmentScanOptions>(scan_options)
++              ->parse_options;
++    }
++#endif
++  }
+   arrow::dataset::FileSystemFactoryOptions options;
+ 
+   std::vector<std::string> uri_vec = ToStringVector(env, uris);
+diff --git a/java/dataset/src/main/java/org/apache/arrow/dataset/file/FileSystemDatasetFactory.java b/java/dataset/src/main/java/org/apache/arrow/dataset/file/FileSystemDatasetFactory.java
+index aa31569..a0b6fb1 100644
+--- a/java/dataset/src/main/java/org/apache/arrow/dataset/file/FileSystemDatasetFactory.java
++++ b/java/dataset/src/main/java/org/apache/arrow/dataset/file/FileSystemDatasetFactory.java
+@@ -17,8 +17,11 @@
+ 
+ package org.apache.arrow.dataset.file;
+ 
++import java.util.Optional;
++
+ import org.apache.arrow.dataset.jni.NativeDatasetFactory;
+ import org.apache.arrow.dataset.jni.NativeMemoryPool;
++import org.apache.arrow.dataset.scanner.FragmentScanOptions;
+ import org.apache.arrow.memory.BufferAllocator;
+ 
+ /**
+@@ -27,21 +30,34 @@ import org.apache.arrow.memory.BufferAllocator;
+ public class FileSystemDatasetFactory extends NativeDatasetFactory {
+ 
+   public FileSystemDatasetFactory(BufferAllocator allocator, NativeMemoryPool memoryPool, FileFormat format,
+-      String uri) {
+-    super(allocator, memoryPool, createNative(format, uri));
++      String uri, Optional<FragmentScanOptions> fragmentScanOptions) {
++    super(allocator, memoryPool, createNative(format, uri, fragmentScanOptions));
++  }
++
++  public FileSystemDatasetFactory(BufferAllocator allocator, NativeMemoryPool memoryPool, FileFormat format,
++                                  String uri) {
++    super(allocator, memoryPool, createNative(format, uri, Optional.empty()));
++  }
++
++  public FileSystemDatasetFactory(BufferAllocator allocator, NativeMemoryPool memoryPool, FileFormat format,
++                                  String[] uris, Optional<FragmentScanOptions> fragmentScanOptions) {
++    super(allocator, memoryPool, createNative(format, uris, fragmentScanOptions));
+   }
+ 
+   public FileSystemDatasetFactory(BufferAllocator allocator, NativeMemoryPool memoryPool, FileFormat format,
+                                   String[] uris) {
+-    super(allocator, memoryPool, createNative(format, uris));
++    super(allocator, memoryPool, createNative(format, uris, Optional.empty()));
+   }
+ 
+-  private static long createNative(FileFormat format, String uri) {
+-    return JniWrapper.get().makeFileSystemDatasetFactory(uri, format.id());
++  private static long createNative(FileFormat format, String uri, Optional<FragmentScanOptions> fragmentScanOptions) {
++    return JniWrapper.get().makeFileSystemDatasetFactory(uri, format.id(),
++        fragmentScanOptions.map(FragmentScanOptions::serialize).orElse(null));
+   }
+ 
+-  private static long createNative(FileFormat format, String[] uris) {
+-    return JniWrapper.get().makeFileSystemDatasetFactory(uris, format.id());
++  private static long createNative(FileFormat format, String[] uris,
++                                   Optional<FragmentScanOptions> fragmentScanOptions) {
++    return JniWrapper.get().makeFileSystemDatasetFactoryWithFiles(uris, format.id(),
++        fragmentScanOptions.map(FragmentScanOptions::serialize).orElse(null));
+   }
+ 
+ }
+diff --git a/java/dataset/src/main/java/org/apache/arrow/dataset/file/JniWrapper.java b/java/dataset/src/main/java/org/apache/arrow/dataset/file/JniWrapper.java
+index c3a1a4e..c3f8e12 100644
+--- a/java/dataset/src/main/java/org/apache/arrow/dataset/file/JniWrapper.java
++++ b/java/dataset/src/main/java/org/apache/arrow/dataset/file/JniWrapper.java
+@@ -17,6 +17,8 @@
+ 
+ package org.apache.arrow.dataset.file;
+ 
++import java.nio.ByteBuffer;
++
+ import org.apache.arrow.dataset.jni.JniLoader;
+ 
+ /**
+@@ -43,7 +45,8 @@ public class JniWrapper {
+    * @return the native pointer of the arrow::dataset::FileSystemDatasetFactory instance.
+    * @see FileFormat
+    */
+-  public native long makeFileSystemDatasetFactory(String uri, int fileFormat);
++  public native long makeFileSystemDatasetFactory(String uri, int fileFormat,
++                                                  ByteBuffer serializedFragmentScanOptions);
+ 
+   /**
+    * Create FileSystemDatasetFactory and return its native pointer. The pointer is pointing to a
+@@ -54,7 +57,8 @@ public class JniWrapper {
+    * @return the native pointer of the arrow::dataset::FileSystemDatasetFactory instance.
+    * @see FileFormat
+    */
+-  public native long makeFileSystemDatasetFactory(String[] uris, int fileFormat);
++  public native long makeFileSystemDatasetFactoryWithFiles(String[] uris, int fileFormat,
++                                                  ByteBuffer serializedFragmentScanOptions);
+ 
+   /**
+    * Write the content in a {@link org.apache.arrow.c.ArrowArrayStream} into files. This internally
+diff --git a/java/dataset/src/main/java/org/apache/arrow/dataset/jni/JniWrapper.java b/java/dataset/src/main/java/org/apache/arrow/dataset/jni/JniWrapper.java
+index 637a3e8..6d63091 100644
+--- a/java/dataset/src/main/java/org/apache/arrow/dataset/jni/JniWrapper.java
++++ b/java/dataset/src/main/java/org/apache/arrow/dataset/jni/JniWrapper.java
+@@ -80,7 +80,8 @@ public class JniWrapper {
+    * @return the native pointer of the arrow::dataset::Scanner instance.
+    */
+   public native long createScanner(long datasetId, String[] columns, ByteBuffer substraitProjection,
+-                                   ByteBuffer substraitFilter, long batchSize, long memoryPool);
++                                   ByteBuffer substraitFilter, long batchSize, long fileFormat,
++                                   ByteBuffer serializedFragmentScanOptions, long memoryPool);
+ 
+   /**
+    * Get a serialized schema from native instance of a Scanner.
+diff --git a/java/dataset/src/main/java/org/apache/arrow/dataset/jni/NativeDataset.java b/java/dataset/src/main/java/org/apache/arrow/dataset/jni/NativeDataset.java
+index d9abad9..3a96fe7 100644
+--- a/java/dataset/src/main/java/org/apache/arrow/dataset/jni/NativeDataset.java
++++ b/java/dataset/src/main/java/org/apache/arrow/dataset/jni/NativeDataset.java
+@@ -17,6 +17,9 @@
+ 
+ package org.apache.arrow.dataset.jni;
+ 
++import java.nio.ByteBuffer;
++
++import org.apache.arrow.dataset.scanner.FragmentScanOptions;
+ import org.apache.arrow.dataset.scanner.ScanOptions;
+ import org.apache.arrow.dataset.source.Dataset;
+ 
+@@ -40,11 +43,18 @@ public class NativeDataset implements Dataset {
+     if (closed) {
+       throw new NativeInstanceReleasedException();
+     }
+-
++    int fileFormat = -1;
++    ByteBuffer serialized = null;
++    if (options.getFragmentScanOptions().isPresent()) {
++      FragmentScanOptions fragmentScanOptions = options.getFragmentScanOptions().get();
++      fileFormat = fragmentScanOptions.fileFormatId();
++      serialized = fragmentScanOptions.serialize();
++    }
+     long scannerId = JniWrapper.get().createScanner(datasetId, options.getColumns().orElse(null),
+         options.getSubstraitProjection().orElse(null),
+         options.getSubstraitFilter().orElse(null),
+-        options.getBatchSize(), context.getMemoryPool().getNativeInstanceId());
++        options.getBatchSize(), fileFormat, serialized,
++        context.getMemoryPool().getNativeInstanceId());
+ 
+     return new NativeScanner(context, scannerId);
+   }
+diff --git a/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/FragmentScanOptions.java b/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/FragmentScanOptions.java
+new file mode 100644
+index 0000000..8acb2b2
+--- /dev/null
++++ b/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/FragmentScanOptions.java
+@@ -0,0 +1,50 @@
++/*
++ * Licensed to the Apache Software Foundation (ASF) under one or more
++ * contributor license agreements.  See the NOTICE file distributed with
++ * this work for additional information regarding copyright ownership.
++ * The ASF licenses this file to You under the Apache License, Version 2.0
++ * (the "License"); you may not use this file except in compliance with
++ * the License.  You may obtain a copy of the License at
++ *
++ *    http://www.apache.org/licenses/LICENSE-2.0
++ *
++ * Unless required by applicable law or agreed to in writing, software
++ * distributed under the License is distributed on an "AS IS" BASIS,
++ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++ * See the License for the specific language governing permissions and
++ * limitations under the License.
++ */
++
++package org.apache.arrow.dataset.scanner;
++
++import java.nio.ByteBuffer;
++import java.util.Map;
++
++import org.apache.arrow.dataset.substrait.util.ConvertUtil;
++
++import io.substrait.proto.Expression;
++
++public interface FragmentScanOptions {
++  String typeName();
++
++  int fileFormatId();
++
++  ByteBuffer serialize();
++
++  /**
++   * serialize the map.
++   *
++   * @param config config map
++   * @return bufer to jni call argument, should be DirectByteBuffer
++   */
++  default ByteBuffer serializeMap(Map<String, String> config) {
++    if (config.isEmpty()) {
++      return null;
++    }
++
++    Expression.Literal literal = ConvertUtil.mapToExpressionLiteral(config);
++    ByteBuffer buf = ByteBuffer.allocateDirect(literal.getSerializedSize());
++    buf.put(literal.toByteArray());
++    return buf;
++  }
++}
+diff --git a/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/ScanOptions.java b/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/ScanOptions.java
+index 995d05a..aad7193 100644
+--- a/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/ScanOptions.java
++++ b/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/ScanOptions.java
+@@ -31,6 +31,8 @@ public class ScanOptions {
+   private final Optional<ByteBuffer> substraitProjection;
+   private final Optional<ByteBuffer> substraitFilter;
+ 
++  private final Optional<FragmentScanOptions> fragmentScanOptions;
++
+   /**
+    * Constructor.
+    * @param columns Projected columns. Empty for scanning all columns.
+@@ -61,6 +63,7 @@ public class ScanOptions {
+     this.columns = columns;
+     this.substraitProjection = Optional.empty();
+     this.substraitFilter = Optional.empty();
++    this.fragmentScanOptions = Optional.empty();
+   }
+ 
+   public ScanOptions(long batchSize) {
+@@ -83,6 +86,10 @@ public class ScanOptions {
+     return substraitFilter;
+   }
+ 
++  public Optional<FragmentScanOptions> getFragmentScanOptions() {
++    return fragmentScanOptions;
++  }
++
+   /**
+    * Builder for Options used during scanning.
+    */
+@@ -91,6 +98,7 @@ public class ScanOptions {
+     private Optional<String[]> columns;
+     private ByteBuffer substraitProjection;
+     private ByteBuffer substraitFilter;
++    private FragmentScanOptions fragmentScanOptions;
+ 
+     /**
+      * Constructor.
+@@ -136,6 +144,18 @@ public class ScanOptions {
+       return this;
+     }
+ 
++    /**
++     * Set the FragmentScanOptions.
++     *
++     * @param fragmentScanOptions scan options
++     * @return the ScanOptions configured.
++     */
++    public Builder fragmentScanOptions(FragmentScanOptions fragmentScanOptions) {
++      Preconditions.checkNotNull(fragmentScanOptions);
++      this.fragmentScanOptions = fragmentScanOptions;
++      return this;
++    }
++
+     public ScanOptions build() {
+       return new ScanOptions(this);
+     }
+@@ -146,5 +166,6 @@ public class ScanOptions {
+     columns = builder.columns;
+     substraitProjection = Optional.ofNullable(builder.substraitProjection);
+     substraitFilter = Optional.ofNullable(builder.substraitFilter);
++    fragmentScanOptions = Optional.ofNullable(builder.fragmentScanOptions);
+   }
+ }
+diff --git a/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/csv/CsvConvertOptions.java b/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/csv/CsvConvertOptions.java
+new file mode 100644
+index 0000000..08e35ed
+--- /dev/null
++++ b/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/csv/CsvConvertOptions.java
+@@ -0,0 +1,51 @@
++/*
++ * Licensed to the Apache Software Foundation (ASF) under one or more
++ * contributor license agreements.  See the NOTICE file distributed with
++ * this work for additional information regarding copyright ownership.
++ * The ASF licenses this file to You under the Apache License, Version 2.0
++ * (the "License"); you may not use this file except in compliance with
++ * the License.  You may obtain a copy of the License at
++ *
++ *    http://www.apache.org/licenses/LICENSE-2.0
++ *
++ * Unless required by applicable law or agreed to in writing, software
++ * distributed under the License is distributed on an "AS IS" BASIS,
++ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++ * See the License for the specific language governing permissions and
++ * limitations under the License.
++ */
++
++package org.apache.arrow.dataset.scanner.csv;
++
++import java.util.Map;
++import java.util.Optional;
++
++import org.apache.arrow.c.ArrowSchema;
++
++public class CsvConvertOptions {
++
++  private final Map<String, String> configs;
++
++  private Optional<ArrowSchema> cSchema = Optional.empty();
++
++  public CsvConvertOptions(Map<String, String> configs) {
++    this.configs = configs;
++  }
++
++  public Optional<ArrowSchema> getArrowSchema() {
++    return cSchema;
++  }
++
++  public Map<String, String> getConfigs() {
++    return configs;
++  }
++
++  public void set(String key, String value) {
++    configs.put(key, value);
++  }
++
++  public void setArrowSchema(ArrowSchema cSchema) {
++    this.cSchema = Optional.of(cSchema);
++  }
++
++}
+diff --git a/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/csv/CsvFragmentScanOptions.java b/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/csv/CsvFragmentScanOptions.java
+new file mode 100644
+index 0000000..88973f0
+--- /dev/null
++++ b/java/dataset/src/main/java/org/apache/arrow/dataset/scanner/csv/CsvFragmentScanOptions.java
+@@ -0,0 +1,97 @@
++/*
++ * Licensed to the Apache Software Foundation (ASF) under one or more
++ * contributor license agreements.  See the NOTICE file distributed with
++ * this work for additional information regarding copyright ownership.
++ * The ASF licenses this file to You under the Apache License, Version 2.0
++ * (the "License"); you may not use this file except in compliance with
++ * the License.  You may obtain a copy of the License at
++ *
++ *    http://www.apache.org/licenses/LICENSE-2.0
++ *
++ * Unless required by applicable law or agreed to in writing, software
++ * distributed under the License is distributed on an "AS IS" BASIS,
++ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++ * See the License for the specific language governing permissions and
++ * limitations under the License.
++ */
++
++package org.apache.arrow.dataset.scanner.csv;
++
++import java.io.Serializable;
++import java.nio.ByteBuffer;
++import java.util.Locale;
++import java.util.Map;
++import java.util.stream.Collectors;
++import java.util.stream.Stream;
++
++import org.apache.arrow.dataset.file.FileFormat;
++import org.apache.arrow.dataset.scanner.FragmentScanOptions;
++
++public class CsvFragmentScanOptions implements Serializable, FragmentScanOptions {
++  private final CsvConvertOptions convertOptions;
++  private final Map<String, String> readOptions;
++  private final Map<String, String> parseOptions;
++
++
++  /**
++   * csv scan options, map to CPP struct CsvFragmentScanOptions.
++   *
++   * @param convertOptions same struct in CPP
++   * @param readOptions same struct in CPP
++   * @param parseOptions same struct in CPP
++   */
++  public CsvFragmentScanOptions(CsvConvertOptions convertOptions,
++                                Map<String, String> readOptions,
++                                Map<String, String> parseOptions) {
++    this.convertOptions = convertOptions;
++    this.readOptions = readOptions;
++    this.parseOptions = parseOptions;
++  }
++
++  public String typeName() {
++    return FileFormat.CSV.name().toLowerCase(Locale.ROOT);
++  }
++
++  /**
++   * File format id.
++   *
++   * @return id
++   */
++  public int fileFormatId() {
++    return FileFormat.CSV.id();
++  }
++
++  /**
++   * Serialize this class to ByteBuffer and then called by jni call.
++   *
++   * @return DirectByteBuffer
++   */
++  public ByteBuffer serialize() {
++    Map<String, String> options = Stream.concat(Stream.concat(readOptions.entrySet().stream(),
++            parseOptions.entrySet().stream()),
++        convertOptions.getConfigs().entrySet().stream()).collect(
++        Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
++
++    if (convertOptions.getArrowSchema().isPresent()) {
++      options.put("column_types", Long.toString(convertOptions.getArrowSchema().get().memoryAddress()));
++    }
++    return serializeMap(options);
++  }
++
++  public static CsvFragmentScanOptions deserialize(String serialized) {
++    throw new UnsupportedOperationException("Not implemented now");
++  }
++
++  public CsvConvertOptions getConvertOptions() {
++    return convertOptions;
++  }
++
++  public Map<String, String> getReadOptions() {
++    return readOptions;
++  }
++
++  public Map<String, String> getParseOptions() {
++    return parseOptions;
++  }
++
++}
+diff --git a/java/dataset/src/main/java/org/apache/arrow/dataset/substrait/util/ConvertUtil.java b/java/dataset/src/main/java/org/apache/arrow/dataset/substrait/util/ConvertUtil.java
+new file mode 100644
+index 0000000..31a4023
+--- /dev/null
++++ b/java/dataset/src/main/java/org/apache/arrow/dataset/substrait/util/ConvertUtil.java
+@@ -0,0 +1,46 @@
++/*
++ * Licensed to the Apache Software Foundation (ASF) under one or more
++ * contributor license agreements.  See the NOTICE file distributed with
++ * this work for additional information regarding copyright ownership.
++ * The ASF licenses this file to You under the Apache License, Version 2.0
++ * (the "License"); you may not use this file except in compliance with
++ * the License.  You may obtain a copy of the License at
++ *
++ *    http://www.apache.org/licenses/LICENSE-2.0
++ *
++ * Unless required by applicable law or agreed to in writing, software
++ * distributed under the License is distributed on an "AS IS" BASIS,
++ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++ * See the License for the specific language governing permissions and
++ * limitations under the License.
++ */
++
++package org.apache.arrow.dataset.substrait.util;
++
++import java.util.Map;
++
++import io.substrait.proto.Expression;
++
++public class ConvertUtil {
++
++  /**
++   * Convert map to substrait Expression.
++   *
++   * @return Substrait Expression
++   */
++  public static Expression.Literal mapToExpressionLiteral(Map<String, String> values) {
++    Expression.Literal.Builder literalBuilder = Expression.Literal.newBuilder();
++    Expression.Literal.Map.KeyValue.Builder keyValueBuilder =
++        Expression.Literal.Map.KeyValue.newBuilder();
++    Expression.Literal.Map.Builder mapBuilder = Expression.Literal.Map.newBuilder();
++    for (Map.Entry<String, String> entry : values.entrySet()) {
++      literalBuilder.setString(entry.getKey());
++      keyValueBuilder.setKey(literalBuilder.build());
++      literalBuilder.setString(entry.getValue());
++      keyValueBuilder.setValue(literalBuilder.build());
++      mapBuilder.addKeyValues(keyValueBuilder.build());
++    }
++    literalBuilder.setMap(mapBuilder.build());
++    return literalBuilder.build();
++  }
++}
+diff --git a/java/dataset/src/test/java/org/apache/arrow/dataset/substrait/TestAceroSubstraitConsumer.java b/java/dataset/src/test/java/org/apache/arrow/dataset/substrait/TestAceroSubstraitConsumer.java
+index 0fba728..e7903b7 100644
+--- a/java/dataset/src/test/java/org/apache/arrow/dataset/substrait/TestAceroSubstraitConsumer.java
++++ b/java/dataset/src/test/java/org/apache/arrow/dataset/substrait/TestAceroSubstraitConsumer.java
+@@ -31,6 +31,9 @@ import java.util.HashMap;
+ import java.util.Map;
+ import java.util.Optional;
+ 
++import org.apache.arrow.c.ArrowSchema;
++import org.apache.arrow.c.CDataDictionaryProvider;
++import org.apache.arrow.c.Data;
+ import org.apache.arrow.dataset.ParquetWriteSupport;
+ import org.apache.arrow.dataset.TestDataset;
+ import org.apache.arrow.dataset.file.FileFormat;
+@@ -38,8 +41,11 @@ import org.apache.arrow.dataset.file.FileSystemDatasetFactory;
+ import org.apache.arrow.dataset.jni.NativeMemoryPool;
+ import org.apache.arrow.dataset.scanner.ScanOptions;
+ import org.apache.arrow.dataset.scanner.Scanner;
++import org.apache.arrow.dataset.scanner.csv.CsvConvertOptions;
++import org.apache.arrow.dataset.scanner.csv.CsvFragmentScanOptions;
+ import org.apache.arrow.dataset.source.Dataset;
+ import org.apache.arrow.dataset.source.DatasetFactory;
++import org.apache.arrow.memory.BufferAllocator;
+ import org.apache.arrow.vector.ipc.ArrowReader;
+ import org.apache.arrow.vector.types.pojo.ArrowType;
+ import org.apache.arrow.vector.types.pojo.Field;
+@@ -49,6 +55,8 @@ import org.junit.ClassRule;
+ import org.junit.Test;
+ import org.junit.rules.TemporaryFolder;
+ 
++import com.google.common.collect.ImmutableMap;
++
+ public class TestAceroSubstraitConsumer extends TestDataset {
+ 
+   @ClassRule
+@@ -457,4 +465,42 @@ public class TestAceroSubstraitConsumer extends TestDataset {
+     substraitExpression.put(decodedSubstrait);
+     return substraitExpression;
+   }
++
++  @Test
++  public void testCsvConvertOptions() throws Exception {
++    final Schema schema = new Schema(Arrays.asList(
++        Field.nullable("Id", new ArrowType.Int(32, true)),
++        Field.nullable("Name", new ArrowType.Utf8()),
++        Field.nullable("Language", new ArrowType.Utf8())
++    ), null);
++    String path = "file://" + getClass().getResource("/").getPath() + "/data/student.csv";
++    BufferAllocator allocator = rootAllocator();
++    try (ArrowSchema cSchema = ArrowSchema.allocateNew(allocator);
++         CDataDictionaryProvider provider = new CDataDictionaryProvider()) {
++      Data.exportSchema(allocator, schema, provider, cSchema);
++      CsvConvertOptions convertOptions = new CsvConvertOptions(ImmutableMap.of("delimiter", ";"));
++      convertOptions.setArrowSchema(cSchema);
++      CsvFragmentScanOptions fragmentScanOptions = new CsvFragmentScanOptions(
++          convertOptions, ImmutableMap.of(), ImmutableMap.of());
++      ScanOptions options = new ScanOptions.Builder(/*batchSize*/ 32768)
++          .columns(Optional.empty())
++          .fragmentScanOptions(fragmentScanOptions)
++          .build();
++      try (
++          DatasetFactory datasetFactory = new FileSystemDatasetFactory(allocator, NativeMemoryPool.getDefault(),
++              FileFormat.CSV, path);
++          Dataset dataset = datasetFactory.finish();
++          Scanner scanner = dataset.newScan(options);
++          ArrowReader reader = scanner.scanBatches()
++      ) {
++        assertEquals(schema.getFields(), reader.getVectorSchemaRoot().getSchema().getFields());
++        int rowCount = 0;
++        while (reader.loadNextBatch()) {
++          assertEquals("[1, 2, 3]", reader.getVectorSchemaRoot().getVector("Id").toString());
++          rowCount += reader.getVectorSchemaRoot().getRowCount();
++        }
++        assertEquals(3, rowCount);
++      }
++    }
++  }
+ }
+diff --git a/java/dataset/src/test/resources/data/student.csv b/java/dataset/src/test/resources/data/student.csv
+new file mode 100644
+index 0000000..3291946
+--- /dev/null
++++ b/java/dataset/src/test/resources/data/student.csv
+@@ -0,0 +1,4 @@
++Id;Name;Language
++1;Juno;Java
++2;Peter;Python
++3;Celin;C++
diff --git a/recipes/arrow/all/patches/15.0.0-0003-dataset_jni_wrapper.patch b/recipes/arrow/all/patches/15.0.0-0003-dataset_jni_wrapper.patch
new file mode 100644
index 000000000..fa4196b80
--- /dev/null
+++ b/recipes/arrow/all/patches/15.0.0-0003-dataset_jni_wrapper.patch
@@ -0,0 +1,61 @@
+diff --git a/java/dataset/src/main/cpp/jni_wrapper.cc b/java/dataset/src/main/cpp/jni_wrapper.cc
+index 36cdd8d..89cdc39 100644
+--- a/java/dataset/src/main/cpp/jni_wrapper.cc
++++ b/java/dataset/src/main/cpp/jni_wrapper.cc
+@@ -28,7 +28,9 @@
+ #include "arrow/dataset/file_csv.h"
+ #include "arrow/filesystem/localfs.h"
+ #include "arrow/filesystem/path_util.h"
++#ifdef ARROW_S3
+ #include "arrow/filesystem/s3fs.h"
++#endif
+ #include "arrow/engine/substrait/util.h"
+ #include "arrow/engine/substrait/serde.h"
+ #include "arrow/engine/substrait/relation.h"
+@@ -140,20 +142,14 @@ class ReserveFromJava : public arrow::dataset::jni::ReservationListener {
+       : vm_(vm), java_reservation_listener_(java_reservation_listener) {}
+ 
+   arrow::Status OnReservation(int64_t size) override {
+-    JNIEnv* env;
+-    if (vm_->GetEnv(reinterpret_cast<void**>(&env), JNI_VERSION) != JNI_OK) {
+-      return arrow::Status::Invalid("JNIEnv was not attached to current thread");
+-    }
++    JNIEnv* env = arrow::dataset::jni::GetEnvOrAttach(vm_);
+     env->CallObjectMethod(java_reservation_listener_, reserve_memory_method, size);
+     RETURN_NOT_OK(arrow::dataset::jni::CheckException(env));
+     return arrow::Status::OK();
+   }
+ 
+   arrow::Status OnRelease(int64_t size) override {
+-    JNIEnv* env;
+-    if (vm_->GetEnv(reinterpret_cast<void**>(&env), JNI_VERSION) != JNI_OK) {
+-      return arrow::Status::Invalid("JNIEnv was not attached to current thread");
+-    }
++    JNIEnv* env = arrow::dataset::jni::GetEnvOrAttach(vm_);
+     env->CallObjectMethod(java_reservation_listener_, unreserve_memory_method, size);
+     RETURN_NOT_OK(arrow::dataset::jni::CheckException(env));
+     return arrow::Status::OK();
+@@ -645,7 +641,9 @@ JNIEXPORT void JNICALL Java_org_apache_arrow_dataset_jni_JniWrapper_releaseBuffe
+ JNIEXPORT void JNICALL Java_org_apache_arrow_dataset_jni_JniWrapper_ensureS3Finalized(
+     JNIEnv* env, jobject) {
+   JNI_METHOD_START
++#ifdef ARROW_S3
+   JniAssertOkOrThrow(arrow::fs::EnsureS3Finalized());
++#endif
+   JNI_METHOD_END()
+ }
+ 
+diff --git a/java/pom.xml b/java/pom.xml
+index 7b207d6..610bda4 100644
+--- a/java/pom.xml
++++ b/java/pom.xml
+@@ -1101,7 +1101,8 @@
+                     -DARROW_JSON=${ARROW_DATASET}
+                     -DARROW_ORC=${ARROW_ORC}
+                     -DARROW_PARQUET=${ARROW_PARQUET}
+-                    -DARROW_S3=ON
++                    -DARROW_S3=OFF
++                    -DARROW_HDFS=ON
+                     -DARROW_SUBSTRAIT=${ARROW_DATASET}
+                     -DARROW_USE_CCACHE=ON
+                     -DCMAKE_BUILD_TYPE=Release
diff --git a/recipes/arrow/all/patches/15.0.0-0004-dataset_cmakelists.patch b/recipes/arrow/all/patches/15.0.0-0004-dataset_cmakelists.patch
new file mode 100644
index 000000000..1844a4a11
--- /dev/null
+++ b/recipes/arrow/all/patches/15.0.0-0004-dataset_cmakelists.patch
@@ -0,0 +1,28 @@
+diff --git a/java/dataset/CMakeLists.txt b/java/dataset/CMakeLists.txt
+index ede3ee7..02db3a5 100644
+--- a/java/dataset/CMakeLists.txt
++++ b/java/dataset/CMakeLists.txt
+@@ -34,11 +34,20 @@ add_jar(arrow_java_jni_dataset_jar
+ add_library(arrow_java_jni_dataset SHARED src/main/cpp/jni_wrapper.cc
+                                           src/main/cpp/jni_util.cc)
+ set_property(TARGET arrow_java_jni_dataset PROPERTY OUTPUT_NAME "arrow_dataset_jni")
++target_include_directories(arrow_java_jni_dataset
++  PRIVATE
++    $<TARGET_PROPERTY:ArrowDataset::arrow_dataset_static,INTERFACE_INCLUDE_DIRECTORIES>
++    $<TARGET_PROPERTY:ArrowSubstrait::arrow_substrait_static,INTERFACE_INCLUDE_DIRECTORIES>
++)
++
++target_compile_definitions(arrow_java_jni_dataset
++  PRIVATE
++    $<TARGET_PROPERTY:ArrowDataset::arrow_dataset_static,INTERFACE_COMPILE_DEFINITIONS>
++    $<TARGET_PROPERTY:ArrowSubstrait::arrow_substrait_static,INTERFACE_COMPILE_DEFINITIONS>
++)
+ target_link_libraries(arrow_java_jni_dataset
+                       arrow_java_jni_dataset_headers
+-                      jni
+-                      ArrowDataset::arrow_dataset_static
+-                      ArrowSubstrait::arrow_substrait_static)
++                      jni)
+ 
+ if(BUILD_TESTING)
+   add_executable(arrow-java-jni-dataset-test src/main/cpp/jni_util_test.cc
diff --git a/recipes/arrow/all/patches/15.0.2-0001-csv-convertercc.patch b/recipes/arrow/all/patches/15.0.2-0001-csv-convertercc.patch
new file mode 100644
index 000000000..be19ecfb5
--- /dev/null
+++ b/recipes/arrow/all/patches/15.0.2-0001-csv-convertercc.patch
@@ -0,0 +1,475 @@
+diff --git a/cpp/src/arrow/csv/converter.cc b/cpp/src/arrow/csv/converter.cc
+index 3825364fa..3969ae8ce 100644
+--- a/cpp/src/arrow/csv/converter.cc
++++ b/cpp/src/arrow/csv/converter.cc
+@@ -28,6 +28,7 @@
+ #include "arrow/array/builder_binary.h"
+ #include "arrow/array/builder_decimal.h"
+ #include "arrow/array/builder_dict.h"
++#include "arrow/array/builder_nested.h"
+ #include "arrow/array/builder_primitive.h"
+ #include "arrow/csv/parser.h"
+ #include "arrow/status.h"
+@@ -113,7 +114,7 @@ struct ValueDecoder {
+                         const ConvertOptions& options)
+       : type_(type), options_(options) {}
+ 
+-  Status Initialize() {
++  virtual Status Initialize() {
+     // TODO no need to build a separate Trie for each instance
+     return InitializeTrie(options_.null_values, &null_trie_);
+   }
+@@ -236,7 +237,7 @@ struct BooleanValueDecoder : public ValueDecoder {
+ 
+   using ValueDecoder::ValueDecoder;
+ 
+-  Status Initialize() {
++  Status Initialize() override {
+     // TODO no need to build separate Tries for each instance
+     RETURN_NOT_OK(InitializeTrie(options_.true_values, &true_trie_));
+     RETURN_NOT_OK(InitializeTrie(options_.false_values, &false_trie_));
+@@ -483,6 +484,16 @@ class ConcreteDictionaryConverter : public DictionaryConverter {
+   using DictionaryConverter::DictionaryConverter;
+ };
+ 
++class ConcreteListConverter : public ListConverter {
++ public:
++  using ListConverter::ListConverter;
++};
++
++class ConcreteMapConverter : public MapConverter {
++ public:
++  using MapConverter::MapConverter;
++};
++
+ //
+ // Concrete Converter for nulls
+ //
+@@ -608,6 +619,306 @@ class TypedDictionaryConverter : public ConcreteDictionaryConverter {
+   int32_t max_cardinality_ = std::numeric_limits<int32_t>::max();
+ };
+ 
++//
++// Concrete Converter factory for lists
++//
++
++template <typename T, typename ValueDecoderType>
++class TypedListConverter : public ConcreteListConverter {
++ public:
++  TypedListConverter(const std::shared_ptr<DataType>& value_type,
++                     const ConvertOptions& options, MemoryPool* pool)
++      : ConcreteListConverter(value_type, options, pool),
++        decoder_(value_type, options_) {}
++
++  Result<std::shared_ptr<Array>> Convert(const BlockParser& parser,
++                                         int32_t col_index) override {
++    using BuilderType = typename TypeTraits<T>::BuilderType;
++    using value_type = typename ValueDecoderType::value_type;
++
++    auto list_separator = parser.getParseOptions().collection_delimiter;
++
++    auto value_builder = std::make_shared<BuilderType>(value_type_, pool_);
++    arrow::ListBuilder list_builder(pool_, value_builder);
++
++    auto visit = [&](const uint8_t* data, uint32_t size, bool quoted) -> Status {
++      if (decoder_.IsNull(data, size, quoted /* quoted */)) {
++        return list_builder.AppendNull();
++      }
++      if (ARROW_PREDICT_FALSE(list_builder.maximum_elements() > max_cardinality_)) {
++        return Status::IndexError("List length exceeded max cardinality");
++      }
++      ARROW_RETURN_NOT_OK(list_builder.Append());
++      std::string str((char*)data, size);
++      std::stringstream ss(str);
++      std::string token;
++      while (std::getline(ss, token, list_separator)) {
++        if (decoder_.IsNull((uint8_t*)token.c_str(), token.size(), quoted /* quoted */)) {
++          ARROW_RETURN_NOT_OK(value_builder->AppendNull());
++          continue;
++        }
++
++        value_type value{};
++        ARROW_RETURN_NOT_OK(
++            decoder_.Decode((uint8_t*)token.c_str(), token.size(), quoted, &value));
++        ARROW_RETURN_NOT_OK(value_builder->Append(value));
++      }
++      return Status::OK();
++    };
++    ARROW_RETURN_NOT_OK(parser.VisitColumn(col_index, visit));
++
++    std::shared_ptr<Array> res;
++    ARROW_RETURN_NOT_OK(list_builder.Finish(&res));
++    return res;
++  }
++
++  void SetMaxCardinality(int32_t max_length) override { max_cardinality_ = max_length; }
++
++ protected:
++  Status Initialize() override {
++    util::InitializeUTF8();
++    return decoder_.Initialize();
++  }
++
++  ValueDecoderType decoder_;
++  int32_t max_cardinality_ = std::numeric_limits<int32_t>::max();
++};
++
++template <typename BuilderType, typename DecoderType>
++arrow::Status appendTypedValue(std::shared_ptr<ValueDecoder> decoder,
++                               std::shared_ptr<ArrayBuilder> builder, const uint8_t* data,
++                               uint32_t size, bool quoted) {
++  using value_type = typename DecoderType::value_type;
++  value_type value{};
++  auto concretedDecoder = std::static_pointer_cast<DecoderType>(decoder);
++  ARROW_RETURN_NOT_OK(concretedDecoder->Decode(data, size, quoted, &value));
++  auto concreteBuilder = std::dynamic_pointer_cast<BuilderType>(builder);
++  ARROW_RETURN_NOT_OK(concreteBuilder->Append(value));
++  return Status::OK();
++}
++
++arrow::Status appendValue(std::shared_ptr<DataType> type,
++                          std::shared_ptr<ValueDecoder> decoder,
++                          std::shared_ptr<ArrayBuilder> builder, const uint8_t* data,
++                          uint32_t size, bool quoted, const ConvertOptions options) {
++  switch (type->id()) {
++    case Type::BOOL:
++      return appendTypedValue<BooleanBuilder, BooleanValueDecoder>(decoder, builder, data,
++                                                                   size, quoted);
++    case Type::INT8:
++      return appendTypedValue<Int8Builder, NumericValueDecoder<Int8Type>>(
++          decoder, builder, data, size, quoted);
++    case Type::INT16:
++      return appendTypedValue<Int16Builder, NumericValueDecoder<Int16Type>>(
++          decoder, builder, data, size, quoted);
++    case Type::INT32:
++      return appendTypedValue<Int32Builder, NumericValueDecoder<Int32Type>>(
++          decoder, builder, data, size, quoted);
++    case Type::INT64:
++      return appendTypedValue<Int64Builder, NumericValueDecoder<Int64Type>>(
++          decoder, builder, data, size, quoted);
++    case Type::FLOAT:
++      return appendTypedValue<FloatBuilder, NumericValueDecoder<FloatType>>(
++          decoder, builder, data, size, quoted);
++    case Type::DOUBLE:
++      return appendTypedValue<DoubleBuilder, NumericValueDecoder<DoubleType>>(
++          decoder, builder, data, size, quoted);
++    case Type::DECIMAL:
++      return appendTypedValue<DecimalBuilder, DecimalValueDecoder>(decoder, builder, data,
++                                                                   size, quoted);
++    case Type::TIMESTAMP:
++      return appendTypedValue<TimestampBuilder, InlineISO8601ValueDecoder>(
++          decoder, builder, data, size, quoted);
++    case Type::FIXED_SIZE_BINARY:
++      return appendTypedValue<FixedSizeBinaryBuilder, FixedSizeBinaryValueDecoder>(
++          decoder, builder, data, size, quoted);
++    case Type::BINARY:
++      return appendTypedValue<BinaryBuilder, BinaryValueDecoder<false>>(
++          decoder, builder, data, size, quoted);
++    case Type::LARGE_BINARY:
++      return appendTypedValue<LargeBinaryBuilder, BinaryValueDecoder<false>>(
++          decoder, builder, data, size, quoted);
++    case Type::STRING:
++      if (options.check_utf8) {
++        return appendTypedValue<StringBuilder, BinaryValueDecoder<true>>(
++            decoder, builder, data, size, quoted);
++      } else {
++        return appendTypedValue<StringBuilder, BinaryValueDecoder<false>>(
++            decoder, builder, data, size, quoted);
++      }
++    case Type::LARGE_STRING:
++      if (options.check_utf8) {
++        return appendTypedValue<LargeStringBuilder, BinaryValueDecoder<true>>(
++            decoder, builder, data, size, quoted);
++      } else {
++        return appendTypedValue<LargeStringBuilder, BinaryValueDecoder<false>>(
++            decoder, builder, data, size, quoted);
++      }
++      break;
++    default:
++      return Status::NotImplemented("Type is not implemented");
++  }
++}
++
++std::shared_ptr<ArrayBuilder> getBuilder(const std::shared_ptr<DataType>& type,
++                                         MemoryPool* pool) {
++  switch (type->id()) {
++    case Type::BOOL:
++      return std::make_shared<BooleanBuilder>(type, pool);
++    case Type::INT8:
++      return std::make_shared<Int8Builder>(type, pool);
++    case Type::INT16:
++      return std::make_shared<Int16Builder>(type, pool);
++    case Type::INT32:
++      return std::make_shared<Int32Builder>(type, pool);
++    case Type::INT64:
++      return std::make_shared<Int64Builder>(type, pool);
++    case Type::FLOAT:
++      return std::make_shared<FloatBuilder>(type, pool);
++    case Type::DOUBLE:
++      return std::make_shared<DoubleBuilder>(type, pool);
++    case Type::DECIMAL:
++      return std::make_shared<Decimal128Builder>(type, pool);
++    case Type::TIMESTAMP:
++      return std::make_shared<TimestampBuilder>(type, pool);
++    case Type::FIXED_SIZE_BINARY:
++      return std::make_shared<FixedSizeBinaryBuilder>(type, pool);
++    case Type::BINARY:
++      return std::make_shared<BinaryBuilder>(type, pool);
++    case Type::LARGE_BINARY:
++      return std::make_shared<LargeBinaryBuilder>(type, pool);
++    case Type::STRING:
++      return std::make_shared<StringBuilder>(type, pool);
++    case Type::LARGE_STRING:
++      return std::make_shared<LargeStringBuilder>(type, pool);
++    default:
++      throw std::runtime_error("Not implemented builder for type");
++  }
++}
++
++std::shared_ptr<ValueDecoder> getDecoder(const std::shared_ptr<DataType>& type,
++                                         const ConvertOptions& options) {
++  switch (type->id()) {
++    case Type::BOOL:
++      return std::make_shared<BooleanValueDecoder>(type, options);
++    case Type::INT8:
++      return std::make_shared<NumericValueDecoder<Int8Type>>(type, options);
++    case Type::INT16:
++      return std::make_shared<NumericValueDecoder<Int16Type>>(type, options);
++    case Type::INT32:
++      return std::make_shared<NumericValueDecoder<Int32Type>>(type, options);
++    case Type::INT64:
++      return std::make_shared<NumericValueDecoder<Int64Type>>(type, options);
++    case Type::FLOAT:
++      return std::make_shared<NumericValueDecoder<FloatType>>(type, options);
++    case Type::DOUBLE:
++      return std::make_shared<NumericValueDecoder<DoubleType>>(type, options);
++    case Type::DECIMAL: {
++      if (options.decimal_point != '.') {
++        throw std::runtime_error("Custom decimal value separator is not supported");
++      }
++      return std::make_shared<DecimalValueDecoder>(type, options);
++    }
++    case Type::TIMESTAMP:
++      return std::make_shared<InlineISO8601ValueDecoder>(type, options);
++    case Type::FIXED_SIZE_BINARY:
++      return std::make_shared<FixedSizeBinaryValueDecoder>(type, options);
++    case Type::BINARY:
++      return std::make_shared<BinaryValueDecoder<false>>(type, options);
++    case Type::LARGE_BINARY:
++      return std::make_shared<BinaryValueDecoder<false>>(type, options);
++    case Type::STRING:
++    case Type::LARGE_STRING:
++
++      if (options.check_utf8) {
++        return std::make_shared<BinaryValueDecoder<true>>(type, options);
++      } else {
++        return std::make_shared<BinaryValueDecoder<false>>(type, options);
++      }
++    default:
++      throw std::runtime_error("Not implemented decoder for type");
++  }
++}
++
++//
++// Concrete Converter factory for lists
++//
++
++class TypedMapConverter : public ConcreteMapConverter {
++ public:
++  TypedMapConverter(const std::shared_ptr<DataType>& key_type,
++                    const std::shared_ptr<DataType>& item_type_,
++                    const ConvertOptions& options, MemoryPool* pool)
++      : ConcreteMapConverter(key_type, item_type_, options, pool),
++        key_decoder_(getDecoder(key_type, options_)),
++        item_decoder_(getDecoder(item_type_, options_)) {}
++
++  Result<std::shared_ptr<Array>> Convert(const BlockParser& parser,
++                                         int32_t col_index) override {
++    auto key_builder = getBuilder(key_type_, pool_);
++    auto item_builder = getBuilder(item_type_, pool_);
++
++    MapBuilder map_builder(pool_, key_builder, item_builder);
++
++    auto parseOptions = parser.getParseOptions();
++    auto map_delimiter = parseOptions.map_key_delimiter;
++    auto list_delimiter = parseOptions.collection_delimiter;
++
++    auto visit = [&](const uint8_t* data, uint32_t size, bool quoted) -> Status {
++      if (key_decoder_->IsNull(data, size, quoted /* quoted */)) {
++        return map_builder.AppendNull();
++      }
++
++      ARROW_RETURN_NOT_OK(map_builder.Append());
++      std::string str((char*)data, size);
++      std::stringstream ss(str);
++      std::string token;
++      while (std::getline(ss, token, list_delimiter)) {
++        if (key_decoder_->IsNull((uint8_t*)token.c_str(), token.size(),
++                                 quoted /* quoted */)) {
++          ARROW_RETURN_NOT_OK(item_builder->AppendNull());
++          continue;
++        }
++
++        auto delimiterLocation = token.find(map_delimiter);
++        if (delimiterLocation == std::string::npos) {
++          return Status::Invalid("Map is not valid");
++        }
++
++        auto key = token.substr(0, delimiterLocation);
++        auto value = token.substr(delimiterLocation + 1);
++
++        ARROW_RETURN_NOT_OK(appendValue(key_type_, key_decoder_, key_builder,
++                                        (const uint8_t*)key.data(), key.size(), quoted,
++                                        options_));
++        ARROW_RETURN_NOT_OK(appendValue(item_type_, item_decoder_, item_builder,
++                                        (const uint8_t*)value.data(), value.size(),
++                                        quoted, options_));
++      }
++      return Status::OK();
++    };
++    ARROW_RETURN_NOT_OK(parser.VisitColumn(col_index, visit));
++
++    std::shared_ptr<Array> res;
++    ARROW_RETURN_NOT_OK(map_builder.Finish(&res));
++    ARROW_RETURN_NOT_OK(res->ValidateFull());
++    return res;
++  }
++
++  void SetMaxCardinality(int32_t max_length) override { max_cardinality_ = max_length; }
++
++ protected:
++  Status Initialize() override {
++    util::InitializeUTF8();
++    ARROW_RETURN_NOT_OK(key_decoder_->Initialize());
++    return item_decoder_->Initialize();
++  }
++
++  std::shared_ptr<ValueDecoder> key_decoder_;
++  std::shared_ptr<ValueDecoder> item_decoder_;
++  int32_t max_cardinality_ = std::numeric_limits<int32_t>::max();
++};
++
+ //
+ // Concrete Converter factory for timestamps
+ //
+@@ -665,6 +976,17 @@ DictionaryConverter::DictionaryConverter(const std::shared_ptr<DataType>& value_
+     : Converter(dictionary(int32(), value_type), options, pool),
+       value_type_(value_type) {}
+ 
++ListConverter::ListConverter(const std::shared_ptr<DataType>& value_type,
++                             const ConvertOptions& options, MemoryPool* pool)
++    : Converter(list(value_type), options, pool), value_type_(value_type) {}
++
++MapConverter::MapConverter(const std::shared_ptr<DataType>& key_type,
++                           const std::shared_ptr<DataType>& item_type_,
++                           const ConvertOptions& options, MemoryPool* pool)
++    : Converter(map(key_type, item_type_), options, pool),
++      key_type_(key_type),
++      item_type_(item_type_) {}
++
+ Result<std::shared_ptr<Converter>> Converter::Make(const std::shared_ptr<DataType>& type,
+                                                    const ConvertOptions& options,
+                                                    MemoryPool* pool) {
+@@ -747,6 +1069,24 @@ Result<std::shared_ptr<Converter>> Converter::Make(const std::shared_ptr<DataTyp
+       return DictionaryConverter::Make(dict_type.value_type(), options, pool);
+     }
+ 
++    case Type::LIST: {
++      try {
++        const auto& list_type = checked_cast<const ListType&>(*type);
++        return ListConverter::Make(list_type.value_type(), options, pool);
++      } catch (const std::exception& e) {
++        return Status::Invalid("Not supported LIST type is provided.", e.what());
++      }
++    }
++
++    case Type::MAP: {
++      try {
++        const auto& map_type = checked_cast<const MapType&>(*type);
++        return MapConverter::Make(map_type.key_type(), map_type.item_type(), options,
++                                  pool);
++      } catch (const std::exception& e) {
++        return Status::Invalid("Not supported MAP type is provided.", e.what());
++      }
++    }
+     default: {
+       return Status::NotImplemented("CSV conversion to ", type->ToString(),
+                                     " is not supported");
+@@ -827,5 +1167,77 @@ Result<std::shared_ptr<DictionaryConverter>> DictionaryConverter::Make(
+   return ptr;
+ }
+ 
++Result<std::shared_ptr<MapConverter>> MapConverter::Make(
++    const std::shared_ptr<DataType>& key_type, const std::shared_ptr<DataType>& item_type,
++    const ConvertOptions& options, MemoryPool* pool) {
++  std::shared_ptr<MapConverter> ptr;
++  ptr.reset(new TypedMapConverter(key_type, item_type, options, pool));
++  RETURN_NOT_OK(ptr->Initialize());
++  return ptr;
++}
++
++Result<std::shared_ptr<ListConverter>> ListConverter::Make(
++    const std::shared_ptr<DataType>& type, const ConvertOptions& options,
++    MemoryPool* pool) {
++  std::shared_ptr<ListConverter> ptr;
++
++  switch (type->id()) {
++#define CONVERTER_CASE(TYPE_ID, TYPE, VALUE_DECODER_TYPE)                             \
++  case TYPE_ID:                                                                       \
++    ptr.reset(new TypedListConverter<TYPE, VALUE_DECODER_TYPE>(type, options, pool)); \
++    break;
++#define REAL_CONVERTER_CASE(TYPE_ID, TYPE_CLASS, DECODER)                            \
++  case TYPE_ID:                                                                      \
++    ptr = MakeRealConverter<ListConverter, TypedListConverter, TYPE_CLASS, DECODER>( \
++        type, options, pool);                                                        \
++    break;
++    CONVERTER_CASE(Type::BOOL, BooleanType, BooleanValueDecoder)
++    CONVERTER_CASE(Type::INT8, Int8Type, NumericValueDecoder<Int8Type>)
++    CONVERTER_CASE(Type::INT16, Int16Type, NumericValueDecoder<Int16Type>)
++    CONVERTER_CASE(Type::INT32, Int32Type, NumericValueDecoder<Int32Type>)
++    CONVERTER_CASE(Type::INT64, Int64Type, NumericValueDecoder<Int64Type>)
++    /* No unsigned sql type, so hard to test, ignoring unsigned types  */
++    CONVERTER_CASE(Type::FLOAT, FloatType, NumericValueDecoder<FloatType>)
++    CONVERTER_CASE(Type::DOUBLE, DoubleType, NumericValueDecoder<DoubleType>)
++    REAL_CONVERTER_CASE(Type::DECIMAL, Decimal128Type, DecimalValueDecoder)
++    CONVERTER_CASE(Type::FIXED_SIZE_BINARY, FixedSizeBinaryType,
++                   FixedSizeBinaryValueDecoder)
++    CONVERTER_CASE(Type::BINARY, BinaryType, BinaryValueDecoder<false>)
++    CONVERTER_CASE(Type::LARGE_BINARY, LargeBinaryType, BinaryValueDecoder<false>)
++    // supporting only ISO-8601
++    CONVERTER_CASE(Type::TIMESTAMP, TimestampType, InlineISO8601ValueDecoder)
++
++    case Type::STRING:
++      if (options.check_utf8) {
++        ptr = std::make_shared<TypedListConverter<StringType, BinaryValueDecoder<true>>>(
++            type, options, pool);
++      } else {
++        ptr = std::make_shared<TypedListConverter<StringType, BinaryValueDecoder<false>>>(
++            type, options, pool);
++      }
++      break;
++
++    case Type::LARGE_STRING:
++      if (options.check_utf8) {
++        ptr = std::make_shared<
++            TypedListConverter<LargeStringType, BinaryValueDecoder<true>>>(type, options,
++                                                                           pool);
++      } else {
++        ptr = std::make_shared<
++            TypedListConverter<LargeStringType, BinaryValueDecoder<false>>>(type, options,
++                                                                            pool);
++      }
++      break;
++    default: {
++      return Status::NotImplemented("CSV list conversion to ", type->ToString(),
++                                    " is not supported");
++    }
++
++#undef CONVERTER_CASE
++  }
++  RETURN_NOT_OK(ptr->Initialize());
++  return ptr;
++}
++
+ }  // namespace csv
+ }  // namespace arrow
diff --git a/recipes/arrow/all/patches/15.0.2-0001-csv-converterh.patch b/recipes/arrow/all/patches/15.0.2-0001-csv-converterh.patch
new file mode 100644
index 000000000..744f58a0e
--- /dev/null
+++ b/recipes/arrow/all/patches/15.0.2-0001-csv-converterh.patch
@@ -0,0 +1,45 @@
+diff --git a/cpp/src/arrow/csv/converter.h b/cpp/src/arrow/csv/converter.h
+index 639f692f2..04832d9d0 100644
+--- a/cpp/src/arrow/csv/converter.h
++++ b/cpp/src/arrow/csv/converter.h
+@@ -78,5 +78,40 @@ class ARROW_EXPORT DictionaryConverter : public Converter {
+   std::shared_ptr<DataType> value_type_;
+ };
+ 
++class ARROW_EXPORT ListConverter : public Converter {
++ public:
++  ListConverter(const std::shared_ptr<DataType>& value_type,
++                      const ConvertOptions& options, MemoryPool* pool);
++
++  // If the list length goes above this value, conversion will fail
++  // with Status::IndexError.
++  virtual void SetMaxCardinality(int32_t max_length) = 0;
++
++  static Result<std::shared_ptr<ListConverter>> Make(
++      const std::shared_ptr<DataType>& value_type, const ConvertOptions& options,
++      MemoryPool* pool = default_memory_pool());
++
++ protected:
++  std::shared_ptr<DataType> value_type_;
++};
++
++class ARROW_EXPORT MapConverter : public Converter {
++ public:
++  MapConverter(const std::shared_ptr<DataType>& key_type, const std::shared_ptr<DataType>& item_type_,
++                      const ConvertOptions& options, MemoryPool* pool);
++
++  // If the map length goes above this value, conversion will fail
++  // with Status::IndexError.
++  virtual void SetMaxCardinality(int32_t max_length) = 0;
++
++  static Result<std::shared_ptr<MapConverter>> Make(
++      const std::shared_ptr<DataType>& key_type, const std::shared_ptr<DataType>& item_type_, const ConvertOptions& options,
++      MemoryPool* pool = default_memory_pool());
++
++ protected:
++  std::shared_ptr<DataType> key_type_;
++  std::shared_ptr<DataType> item_type_;
++};
++
+ }  // namespace csv
+ }  // namespace arrow
diff --git a/recipes/arrow/all/patches/15.0.2-0001-csv-optionsh.patch b/recipes/arrow/all/patches/15.0.2-0001-csv-optionsh.patch
new file mode 100644
index 000000000..00522e139
--- /dev/null
+++ b/recipes/arrow/all/patches/15.0.2-0001-csv-optionsh.patch
@@ -0,0 +1,15 @@
+diff --git a/cpp/src/arrow/csv/options.h b/cpp/src/arrow/csv/options.h
+index 7723dcedc..b62cf88f0 100644
+--- a/cpp/src/arrow/csv/options.h
++++ b/cpp/src/arrow/csv/options.h
+@@ -44,6 +44,10 @@ struct ARROW_EXPORT ParseOptions {
+ 
+   /// Field delimiter
+   char delimiter = ',';
++  /// Collection delimiter
++  char collection_delimiter = ';';
++  /// Map key delimiter
++  char map_key_delimiter = ':';
+   /// Whether quoting is used
+   bool quoting = true;
+   /// Quoting character (if `quoting` is true)
diff --git a/recipes/arrow/all/patches/15.0.2-0001-csv-parsercc.patch b/recipes/arrow/all/patches/15.0.2-0001-csv-parsercc.patch
new file mode 100644
index 000000000..cc9e76d41
--- /dev/null
+++ b/recipes/arrow/all/patches/15.0.2-0001-csv-parsercc.patch
@@ -0,0 +1,23 @@
+diff --git a/cpp/src/arrow/csv/parser.cc b/cpp/src/arrow/csv/parser.cc
+index da3472a9d..690e94119 100644
+--- a/cpp/src/arrow/csv/parser.cc
++++ b/cpp/src/arrow/csv/parser.cc
+@@ -192,6 +192,8 @@ class BlockParserImpl {
+         max_num_rows_(max_num_rows),
+         batch_(num_cols) {}
+ 
++  const ParseOptions& getParseOptions() const { return options_; }
++
+   const DataBatch& parsed_batch() const { return batch_; }
+ 
+   int64_t first_row_num() const { return first_row_; }
+@@ -698,5 +700,9 @@ int32_t SkipRows(const uint8_t* data, uint32_t size, int32_t num_rows,
+   return skipped_rows;
+ }
+ 
++const ParseOptions& BlockParser::getParseOptions() const {
++  return impl_->getParseOptions();
++}
++
+ }  // namespace csv
+ }  // namespace arrow
diff --git a/recipes/arrow/all/patches/15.0.2-0001-csv-parserh.patch b/recipes/arrow/all/patches/15.0.2-0001-csv-parserh.patch
new file mode 100644
index 000000000..6f9213cc7
--- /dev/null
+++ b/recipes/arrow/all/patches/15.0.2-0001-csv-parserh.patch
@@ -0,0 +1,13 @@
+diff --git a/cpp/src/arrow/csv/parser.h b/cpp/src/arrow/csv/parser.h
+index c73e52ce8..fdbb23f90 100644
+--- a/cpp/src/arrow/csv/parser.h
++++ b/cpp/src/arrow/csv/parser.h
+@@ -218,6 +218,8 @@ class ARROW_EXPORT BlockParser {
+     return parsed_batch().VisitLastRow(std::forward<Visitor>(visit));
+   }
+ 
++  const ParseOptions& getParseOptions() const;
++
+  protected:
+   std::unique_ptr<BlockParserImpl> impl_;
+ 
diff --git a/recipes/arrow/config.yml b/recipes/arrow/config.yml
index 254dcc7ab..8c9a1580f 100644
--- a/recipes/arrow/config.yml
+++ b/recipes/arrow/config.yml
@@ -13,6 +13,8 @@ versions:
     folder: all
   "16.1.0":
     folder: all
+  "15.0.1-oss":
+    folder: all
   "15.0.0":
     folder: all
   "14.0.2":
